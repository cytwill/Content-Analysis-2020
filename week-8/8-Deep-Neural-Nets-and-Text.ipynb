{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week - 8 - Deep Neural Nets and Text\n",
    "\n",
    "In this week we introduce the use of Deep Neural Networks to work with text. We have already seen some uses of neural networks for text in our classification HW, where we used a simple neural network--the one-layer perceptron--to classify text. It performed quite well, but comes up short in more sophisticated classification tasks, such as in predicting intent. We have also seen slightly deeper, 2-level neural nets in the form of word embeddings such as Word2Vec. While they work well, they have some drawbacks, such as representing words with multiple meanings in a singular space. \n",
    "\n",
    "BERT, which is a language model built using bidirectional encoders, allows us to take advantage of a powerful pre-trained model which we can then use to perform our own tasks based on data we analyze. \n",
    "\n",
    "In this notebook we use ```huggingface/transformers```, a python package that allows for easy interface to use pre-trained BERT models. It is built using Tensorflow and PyTorch, two computational graph packages which are built specifically for creating powerful neural networks. We will also be introducing Keras, which allows us to easily build Neural Networks in an abstracted way. Keras is a popular way to understand how we can stack layers to create such Neural Networks, but to reach state-of-the-art results we will stick with using BERT and similar models that can be tuned to extremely high performance on specific language understanding and generation tasks.\n",
    "\n",
    "To demonstrate this, we begin by using the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). We will also use BERT by learning how to extract embeddings from such a model and use it to semantically probe sentences. There are a number of new packages and methods we will be using so be sure to update lucem_illud_2020.\n",
    "\n",
    "## NOTE\n",
    "\n",
    "This notebook **requires** GPUs for training models in section 1 and section 3. To train models, please use this [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) to create the models. Note that I have only given you view access: please create your own colab file to train your models, using the code and instructions I have given in the Colab file. So while you have to do the homework on this notebook, the models which you will train should be done on Google Colab, which has GPU access. If you happen to have GPU access on your personal machines or some other way to train the models, you are welcome to do that too.\n",
    "\n",
    "Note that if you run the computationally intensive models on your local computer they will take a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # pip install torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig # pip install tranformers==2.4.1\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lucem_illud_2020 # pip install -U git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoLA Dataset and pre-processing\n",
    "\n",
    "We start with loading our dataset and pre-processing it. The pre-processing follows similar steps as we have done in the past, but we will be using pre-written modules offered by the transformers package. These are some of the things we have to take care of when using this particular BERT model.\n",
    "\n",
    "    -special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
    "    -tokens that conforms with the fixed vocabulary used in BERT\n",
    "    -token IDs from BERT’s tokenizer\n",
    "    -mask IDs to indicate which elements in the sequence are tokens and which are padding elements\n",
    "    -segment IDs used to distinguish different sentences\n",
    "    -positional embeddings used to show token position within the sequence\n",
    "\n",
    "\n",
    "We will be using parts of the code from [this notebook](https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=BJR6t_gCQe_x) which walks us through the process of using a pre-trained BERT model. The interface to use these models comes from the package [huggingface/transformers](https://github.com/huggingface/transformers). \n",
    "\n",
    "We start by setting up our GPU if we can - this may not work on your machine, so it has been commented out.\n",
    "\n",
    "An aside: check out this tutorial too - https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if gpu:\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "\n",
    "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
    "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a747af3fc764b9ba53bc4dd21ebc894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenize the first sentence:\n",
      "['[CLS]', 'our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8551, 128)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(attention_masks).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2020, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2020, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101, 14207,  2481, ...,     0,     0,     0],\n",
       "       [  101,  3021,  1998, ...,     0,     0,     0],\n",
       "       [  101,  2008,  1005, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101, 27036,  2003, ...,     0,     0,     0],\n",
       "       [  101,  1996,  2210, ...,     0,     0,     0],\n",
       "       [  101,  2984,  2741, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Deep Neural Nets\n",
    "\n",
    "A popular, simplified package for introducing deep neural networks is [Keras](https://keras.io). It is a high level package in that we don't bother with every detail or hyper-parameter associated with the neural network (e.g., regularizers), and can stack on layers directly. For a rapid tutorial on neural networks for text such as the LSTM or the Recurrent Neural Network, Colah's blog is a great start. [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is an article on LSTMs, and if you'd like to  learn about RNN, Andrej Karpathy does a great job in [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), in addition to our reading from the newest online verion of Jurafsky & Martin's review of deep learning methods in their book on speech and language processing, chapters [6,7,9,10](https://web.stanford.edu/~jurafsky/slp3/), and the [*Deep Learning*](https://www.deeplearningbook.org/) book by Goodfellow, Bengio & Courville.\n",
    "\n",
    "In the following cells we build a basic deep net that has an embedding layer and an LSTM to perform classification. This is to illustrate the process of using Keras, which is a very popular library for such work. It may not yield state of the art performance because it constrains the hyperparameters you can tune, but is nonetheless an useful tool and works well on some datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = tokenizer.vocab_size\n",
    "embedding_dim = 32\n",
    "unit = 100\n",
    "no_labels = len(np.unique(train_labels))\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm.add(LSTM(unit))\n",
    "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
    "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "F:\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7695/7695 [==============================] - 24s 3ms/step - loss: 0.6125 - accuracy: 0.7010\n",
      "Epoch 2/10\n",
      "7695/7695 [==============================] - 24s 3ms/step - loss: 0.6100 - accuracy: 0.7038\n",
      "Epoch 3/10\n",
      "7695/7695 [==============================] - 24s 3ms/step - loss: 0.6083 - accuracy: 0.7038\n",
      "Epoch 4/10\n",
      "7695/7695 [==============================] - 25s 3ms/step - loss: 0.6091 - accuracy: 0.7038\n",
      "Epoch 5/10\n",
      "7695/7695 [==============================] - 25s 3ms/step - loss: 0.6091 - accuracy: 0.7038\n",
      "Epoch 6/10\n",
      "7695/7695 [==============================] - 25s 3ms/step - loss: 0.6079 - accuracy: 0.7038\n",
      "Epoch 7/10\n",
      "7695/7695 [==============================] - 25s 3ms/step - loss: 0.6086 - accuracy: 0.7038\n",
      "Epoch 8/10\n",
      "7695/7695 [==============================] - 25s 3ms/step - loss: 0.6082 - accuracy: 0.7038\n",
      "Epoch 9/10\n",
      "7695/7695 [==============================] - 25s 3ms/step - loss: 0.6086 - accuracy: 0.7038\n",
      "Epoch 10/10\n",
      "7695/7695 [==============================] - 25s 3ms/step - loss: 0.6076 - accuracy: 0.7038\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
    "                              epochs=10,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of this model isn't terrible, but it still hovers around 70%. Below there is code for a slightly modified neural network - how does this one perform? Note that in this model, I have added another LSTM layer. You are encouraged to explore the [Keras documentaion](https://keras.io/layers/about-keras-layers/) to explore what kind of layers you can add and how they change performances for different tasks. Different kinds of losses, optimizers, activations and layers can change the flavour of your net dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm2 = Sequential()\n",
    "model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
    "model_lstm2.add(LSTM(unit))\n",
    "model_lstm2.add(LSTM(unit))\n",
    "model_lstm2.add(Dense(1, activation='sigmoid'))\n",
    "model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_lstm2 = model_lstm2.fit(input_data_train, labels, epochs=10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On with BERT!\n",
    "\n",
    "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how a bidirectional transformer embedding like BERT might do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading our Models\n",
    "\n",
    "### Train Model\n",
    "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
    "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
    "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "\n",
    "### Structure of Fine-Tuning Model\n",
    "\n",
    "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
    "\n",
    "### The Fine-Tuning Process\n",
    "\n",
    "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
    "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
    "\n",
    "Credit to Michel Kana's [tutorial](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03) and the [tutorial](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=GuE5BqICAne2) by Chris McCormick and Nick Ryan who describe the workings of BERT and the way it is used by the ```transformers``` package. \n",
    "\n",
    "## WARNING: SHIFT TO A GPU ENABLED MACHINE (e.g., Google Colab)\n",
    "\n",
    "Note that you only want to run the following code if you have a GPU. Otherwise, rerun the **same** cells we just ran on the Colab file to train your model, download it to your local, and load it by running\n",
    "```model = BertForSequenceClassification.from_pretrained(\"my_model_directory\", num_labels=2)```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following cell can take upto 12 hours or longer if run without a GPU. The [Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) demonstrates how to fine-tune models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# # This training code is based on the `run_glue.py` script here:\n",
    "# # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# # Set the seed value all over the place to make this reproducible.\n",
    "# seed_val = 42\n",
    "\n",
    "# random.seed(seed_val)\n",
    "# np.random.seed(seed_val)\n",
    "# torch.manual_seed(seed_val)\n",
    "# torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# # Store the average loss after each epoch so we can plot them.\n",
    "# loss_values = []\n",
    "\n",
    "# # For each epoch...\n",
    "# for epoch_i in range(0, epochs):\n",
    "    \n",
    "#     # ========================================\n",
    "#     #               Training\n",
    "#     # ========================================\n",
    "    \n",
    "#     # Perform one full pass over the training set.\n",
    "\n",
    "#     print(\"\")\n",
    "#     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "#     print('Training...')\n",
    "\n",
    "#     # Measure how long the training epoch takes.\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     # Reset the total loss for this epoch.\n",
    "#     total_loss = 0\n",
    "\n",
    "#     # Put the model into training mode. Don't be mislead--the call to \n",
    "#     # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "#     # `dropout` and `batchnorm` layers behave differently during training\n",
    "#     # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "#     model.train()\n",
    "\n",
    "#     # For each batch of training data...\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "#         # Progress update every 40 batches.\n",
    "#         if step % 40 == 0 and not step == 0:\n",
    "#             # Calculate elapsed time in minutes.\n",
    "#             elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "#             # Report progress.\n",
    "#             print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "#         # Unpack this training batch from our dataloader. \n",
    "#         #\n",
    "#         # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "#         # `to` method.\n",
    "#         #\n",
    "#         # `batch` contains three pytorch tensors:\n",
    "#         #   [0]: input ids \n",
    "#         #   [1]: attention masks\n",
    "#         #   [2]: labels \n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "#         b_labels = batch[2].to(device)\n",
    "\n",
    "#         # Always clear any previously calculated gradients before performing a\n",
    "#         # backward pass. PyTorch doesn't do this automatically because \n",
    "#         # accumulating the gradients is \"convenient while training RNNs\". \n",
    "#         # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "#         model.zero_grad()        \n",
    "\n",
    "#         # Perform a forward pass (evaluate the model on this training batch).\n",
    "#         # This will return the loss (rather than the model output) because we\n",
    "#         # have provided the `labels`.\n",
    "#         # The documentation for this `model` function is here: \n",
    "#         # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "#         outputs = model(b_input_ids, \n",
    "#                     token_type_ids=None, \n",
    "#                     attention_mask=b_input_mask, \n",
    "#                     labels=b_labels)\n",
    "        \n",
    "#         # The call to `model` always returns a tuple, so we need to pull the \n",
    "#         # loss value out of the tuple.\n",
    "#         loss = outputs[0]\n",
    "\n",
    "#         # Accumulate the training loss over all of the batches so that we can\n",
    "#         # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "#         # single value; the `.item()` function just returns the Python value \n",
    "#         # from the tensor.\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         # Perform a backward pass to calculate the gradients.\n",
    "#         loss.backward()\n",
    "\n",
    "#         # Clip the norm of the gradients to 1.0.\n",
    "#         # This is to help prevent the \"exploding gradients\" problem.\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "#         # Update parameters and take a step using the computed gradient.\n",
    "#         # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "#         # modified based on their gradients, the learning rate, etc.\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the learning rate.\n",
    "#         scheduler.step()\n",
    "\n",
    "#     # Calculate the average loss over the training data.\n",
    "#     avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "#     # Store the loss value for plotting the learning curve.\n",
    "#     loss_values.append(avg_train_loss)\n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "#     print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "#     # ========================================\n",
    "#     #               Validation\n",
    "#     # ========================================\n",
    "#     # After the completion of each training epoch, measure our performance on\n",
    "#     # our validation set.\n",
    "\n",
    "#     print(\"\")\n",
    "#     print(\"Running Validation...\")\n",
    "\n",
    "#     t0 = time.time()\n",
    "\n",
    "#     # Put the model in evaluation mode--the dropout layers behave differently\n",
    "#     # during evaluation.\n",
    "#     model.eval()\n",
    "\n",
    "#     # Tracking variables \n",
    "#     eval_loss, eval_accuracy = 0, 0\n",
    "#     nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "#     # Evaluate data for one epoch\n",
    "#     for batch in validation_dataloader:\n",
    "        \n",
    "#         # Add batch to GPU\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "#         # Unpack the inputs from our dataloader\n",
    "#         b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "#         # Telling the model not to compute or store gradients, saving memory and\n",
    "#         # speeding up validation\n",
    "#         with torch.no_grad():        \n",
    "\n",
    "#             # Forward pass, calculate logit predictions.\n",
    "#             # This will return the logits rather than the loss because we have\n",
    "#             # not provided labels.\n",
    "#             # token_type_ids is the same as the \"segment ids\", which \n",
    "#             # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "#             # The documentation for this `model` function is here: \n",
    "#             # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "#             outputs = model(b_input_ids, \n",
    "#                             token_type_ids=None, \n",
    "#                             attention_mask=b_input_mask)\n",
    "        \n",
    "#         # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "#         # values prior to applying an activation function like the softmax.\n",
    "#         logits = outputs[0]\n",
    "\n",
    "#         # Move logits and labels to CPU\n",
    "#         logits = logits.detach().cpu().numpy()\n",
    "#         label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "#         # Calculate the accuracy for this batch of test sentences.\n",
    "#         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "#         # Accumulate the total accuracy.\n",
    "#         eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "#         # Track the number of batches\n",
    "#         nb_eval_steps += 1\n",
    "\n",
    "#     # Report the final accuracy for this validation run.\n",
    "#     print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "#     print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "# print(\"\")\n",
    "# print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Use plot styling from seaborn.\n",
    "# sns.set(style='darkgrid')\n",
    "\n",
    "# # Increase the plot size and font size.\n",
    "# sns.set(font_scale=1.5)\n",
    "# plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# # Plot the learning curve.\n",
    "# plt.plot(loss_values, 'b-o')\n",
    "\n",
    "# # Label the plot.\n",
    "# plt.title(\"Training loss\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
    "\n",
    "Once you tune your model on Colab (or on your own machine if you decided to do that instead), you load it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"model_save\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"../data/cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Create sentence and label lists\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "prediction_inputs = torch.tensor(input_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)\n",
    "prediction_labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 516 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive samples: 354 of 516 (68.60%)\n"
     ]
    }
   ],
   "source": [
    "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhargavvader/open_source/Content-Analysis-2020/venv/lib/python3.5/site-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "    matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.049286405809014416,\n",
       " 0.014456362470655182,\n",
       " 0.4732058754737091,\n",
       " 0.4414147946478204,\n",
       " 0.44440090347500916,\n",
       " 0.7410010097502685,\n",
       " 0.6201736729460423,\n",
       " 0.47519096331149147,\n",
       " 1.0,\n",
       " 0.5659164584181102,\n",
       " 0.7679476477883045,\n",
       " 0.647150228929434,\n",
       " 0.8150678894028793,\n",
       " 0.647150228929434,\n",
       " 0.3268228676411533,\n",
       " 0.5844155844155844,\n",
       " 0.0]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.550\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty good performance. Note that we used [Matthews Correlation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) to meausure this. It ranges from -1 to 1, with +1 being the best. The Google BERT model has a similar score too, so this model performed quite well. It took a long time though, approximately a day with no GPU. It would be significantly faster if a CUDA enabled machine ran this. Hence, we recommend that you run this on the Collab notebook.\n",
    "\n",
    "The following lines save the model to disk, if you would like to: note that we ran this in the colab file to save it to disk there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# output_dir = './model_save/'\n",
    "\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# # They can then be reloaded using `from_pretrained()`\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "# model_to_save.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# # Good practice: save your training arguments together with the trained model\n",
    "# # torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) and also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different?\n",
    "\n",
    "<span style=\"color:red\">***Stretch***</span>: <span style=\"color:red\">Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading my dataset and pre-processing\n",
    "my_df = pd.read_csv('train_preprocessed.csv',nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[CLS], explanation, why, the, edit, ##s, made...\n",
       "1    [[CLS], d, aw, ##w, he, matches, this, backgro...\n",
       "2    [[CLS], hey, man, i, m, really, not, trying, t...\n",
       "3    [[CLS], more, i, can, t, make, any, real, sugg...\n",
       "4    [[CLS], you, sir, are, my, hero, any, chance, ...\n",
       "Name: tagged_tokens, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "my_df['tagged_tokens'] = my_df['comment_text'].apply(lambda x: my_tokenizer.tokenize(\"[CLS] \"+ x +\" [SEP]\"))\n",
    "my_df['tagged_tokens'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [101, 7526, 2339, 1996, 10086, 2015, 2081, 210...\n",
       "1    [101, 1040, 22091, 2860, 2002, 3503, 2023, 428...\n",
       "2    [101, 4931, 2158, 1045, 1049, 2428, 2025, 2667...\n",
       "3    [101, 2062, 1045, 2064, 1056, 2191, 2151, 2613...\n",
       "4    [101, 2017, 2909, 2024, 2026, 5394, 2151, 3382...\n",
       "Name: input_ids, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "MAX_LEN = 128\n",
    "my_input_ids = [my_tokenizer.convert_tokens_to_ids(x) for x in my_df['tagged_tokens'].values]\n",
    "# Pad my input tokens\n",
    "my_df['input_ids'] = list(pad_sequences(my_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\"))\n",
    "my_df['input_ids'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
       "1    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
       "2    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
       "3    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
       "4    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
       "Name: masks, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_df['masks'] = my_df['input_ids'].apply(lambda x: [float(i>0) for i in x])\n",
    "my_df['masks'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "my_train_inputs, my_validation_inputs, my_train_labels, my_validation_labels = train_test_split(\n",
    "    my_df['input_ids'], my_df['toxic'], random_state=2020, test_size=0.2)\n",
    "my_train_masks, my_validation_masks, _, _ = train_test_split(\n",
    "    my_df['masks'], my_df['toxic'], random_state=2020, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,030,106\n",
      "Trainable params: 1,030,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_lstm_1 = Sequential()\n",
    "my_lstm_1.add(Embedding(my_tokenizer.vocab_size, 32, input_length=MAX_LEN))\n",
    "my_lstm_1.add(LSTM(100))\n",
    "my_lstm_1.add(Dense(len(np.unique(my_train_labels)), activation='softmax'))\n",
    "my_lstm_1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "my_lstm_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.0366 - accuracy: 0.9893\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.0263 - accuracy: 0.9925\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.0197 - accuracy: 0.9952\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.0136 - accuracy: 0.9971\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.0097 - accuracy: 0.9976\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.0064 - accuracy: 0.9989\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 0.0047 - accuracy: 0.9992\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 26s 3ms/step - loss: 0.0150 - accuracy: 0.9967\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.0047 - accuracy: 0.9992\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 25s 3ms/step - loss: 0.0039 - accuracy: 0.9994\n"
     ]
    }
   ],
   "source": [
    "lstm_1_history = my_lstm_1.fit(np.array([list(x) for x in my_train_inputs.values]), \n",
    "                               np.array([x for x in my_train_labels.values]), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows that the classification accuracy for my corpora is already disirable, up to 99% (after two rounds of training). What if we add one layer or cutting off the number of embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 128, 32)           976704    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 128, 100)          53200     \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 1,110,506\n",
      "Trainable params: 1,110,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_lstm_2 = Sequential()\n",
    "my_lstm_2.add(Embedding(my_tokenizer.vocab_size, 32, input_length=MAX_LEN))\n",
    "my_lstm_2.add(LSTM(100,return_sequences=True))\n",
    "my_lstm_2.add(LSTM(100))\n",
    "my_lstm_2.add(Dense(len(np.unique(my_train_labels)), activation='softmax'))\n",
    "my_lstm_2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "my_lstm_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 45s 6ms/step - loss: 0.2833 - accuracy: 0.9061\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.2921 - accuracy: 0.9085\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.2931 - accuracy: 0.9096\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.2849 - accuracy: 0.9078\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 50s 6ms/step - loss: 0.1647 - accuracy: 0.9402\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.0882 - accuracy: 0.9721\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.0554 - accuracy: 0.9846\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0449 - accuracy: 0.9883\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 48s 6ms/step - loss: 0.0328 - accuracy: 0.9912\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 49s 6ms/step - loss: 0.0176 - accuracy: 0.9956\n"
     ]
    }
   ],
   "source": [
    "lstm_2_history = my_lstm_2.fit(np.array([list(x) for x in my_train_inputs.values]), \n",
    "                               np.array([x for x in my_train_labels.values]), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result suggests that adding a LSTM layer does not improve the accuracy of the classification. In the first several epoch, we can even see that the accuracy is lower than model 1. Maybe this is a sign of overfitting after adding the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 128, 16)           488352    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 100)               46800     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 535,354\n",
      "Trainable params: 535,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decrease the number of embeddings\n",
    "my_lstm_3 = Sequential()\n",
    "my_lstm_3.add(Embedding(my_tokenizer.vocab_size, 16, input_length=MAX_LEN))\n",
    "my_lstm_3.add(LSTM(100))\n",
    "my_lstm_3.add(Dense(len(np.unique(my_train_labels)), activation='softmax'))\n",
    "my_lstm_3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "my_lstm_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 21s 3ms/step - loss: 0.2633 - accuracy: 0.9204\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.2578 - accuracy: 0.9195 0s - loss: 0.2579 - accuracy: \n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 22s 3ms/step - loss: 0.2606 - accuracy: 0.9199 1s - l\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 22s 3ms/step - loss: 0.2170 - accuracy: 0.9260\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.2208 - accuracy: 0.9349\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.2136 - accuracy: 0.93 - 22s 3ms/step - loss: 0.2135 - accuracy: 0.9308\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 22s 3ms/step - loss: 0.2424 - accuracy: 0.9075\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.2608 - accuracy: 0.9122\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 24s 3ms/step - loss: 0.2034 - accuracy: 0.9401\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 22s 3ms/step - loss: 0.2203 - accuracy: 0.9191\n"
     ]
    }
   ],
   "source": [
    "lstm_3_history = my_lstm_3.fit(np.array([list(x) for x in my_train_inputs.values]), \n",
    "                               np.array([x for x in my_train_labels.values]), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that removing word embeddings have a stronger effect on the classification accuracy. Generally, more embeddings are likely to lead to a better result since they capture more variance between the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try BERT\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "bret_train_inputs = torch.tensor(np.array([list(x) for x in my_train_inputs.values]),dtype=torch.long)\n",
    "bret_validation_inputs = torch.tensor(np.array([list(x) for x in my_validation_inputs.values]),dtype=torch.long)\n",
    "bret_train_labels = torch.tensor(np.array([x for x in my_train_labels.values]),dtype=torch.long)\n",
    "bret_validation_labels = torch.tensor(np.array([x for x in my_validation_labels.values]),dtype=torch.long)\n",
    "bret_train_masks = torch.tensor(np.array([x for x in my_train_masks.values]),dtype=torch.long)\n",
    "bret_validation_masks = torch.tensor(np.array([x for x in my_validation_masks.values]),dtype=torch.long)\n",
    "\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "bret_train_data = TensorDataset(bret_train_inputs, bret_train_masks, bret_train_labels)\n",
    "bret_train_sampler = RandomSampler(bret_train_data)\n",
    "bret_train_dataloader = DataLoader(bret_train_data, sampler=bret_train_sampler, batch_size=batch_size)\n",
    "\n",
    "bret_validation_data = TensorDataset(bret_validation_inputs, bret_validation_masks, bret_validation_labels)\n",
    "bret_validation_sampler = SequentialSampler(bret_validation_data)\n",
    "bret_validation_dataloader = DataLoader(bret_validation_data, sampler=bret_validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(bret_train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(bret_train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bret_train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(bret_train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in bret_validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        \n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading trained model\n",
    "my_trained_model = BertForSequenceClassification.from_pretrained(\"my_bret\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>insult</th>\n",
       "      <th>obscene</th>\n",
       "      <th>set</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>threat</th>\n",
       "      <th>toxic</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>will do buddy but what is this thing about not...</td>\n",
       "      <td>1a7d550fec6e9777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi redrose and apologies for delay  here is a ...</td>\n",
       "      <td>1a7d7c88372e5668</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>catharine beecher hey i love catharine beecher...</td>\n",
       "      <td>1a7dcdc88c79c37f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you were really interested in good faith ed...</td>\n",
       "      <td>1a7faf5ba454dd05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>islamofascism the term islamofascism is  in m...</td>\n",
       "      <td>1a825b0d694174cf</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text                id  \\\n",
       "0  will do buddy but what is this thing about not...  1a7d550fec6e9777   \n",
       "1  hi redrose and apologies for delay  here is a ...  1a7d7c88372e5668   \n",
       "2  catharine beecher hey i love catharine beecher...  1a7dcdc88c79c37f   \n",
       "3  if you were really interested in good faith ed...  1a7faf5ba454dd05   \n",
       "4   islamofascism the term islamofascism is  in m...  1a825b0d694174cf   \n",
       "\n",
       "   identity_hate  insult  obscene    set  severe_toxic  threat  toxic  \\\n",
       "0            0.0     0.0      0.0  train           0.0     0.0    0.0   \n",
       "1            0.0     0.0      0.0  train           0.0     0.0    0.0   \n",
       "2            0.0     0.0      0.0  train           0.0     0.0    0.0   \n",
       "3            0.0     0.0      0.0  train           0.0     0.0    0.0   \n",
       "4            0.0     0.0      0.0  train           0.0     0.0    0.0   \n",
       "\n",
       "   toxicity  \n",
       "0       0.0  \n",
       "1       0.0  \n",
       "2       0.0  \n",
       "3       0.0  \n",
       "4       0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test_df = pd.read_csv('train_preprocessed.csv',skiprows=10000, nrows=2000, header=None)\n",
    "my_test_df.columns = my_df.columns\n",
    "my_test_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentence and label lists\n",
    "test_sentences = my_test_df.comment_text.values\n",
    "test_labels = my_test_df.toxic.values\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "test_input_ids = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in test_sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    encoded_sent = my_tokenizer.encode(sent,add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    test_input_ids.append(encoded_sent)\n",
    "\n",
    "# Pad our input tokens\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=128, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 128)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "test_attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in test_input_ids:\n",
    "    test_seq_mask = [float(i>0) for i in seq]\n",
    "    test_attention_masks.append(test_seq_mask) \n",
    "\n",
    "# Convert to tensors.\n",
    "test_inputs = torch.tensor(test_input_ids,dtype=torch.long)\n",
    "test_masks = torch.tensor(test_attention_masks,dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels,dtype=torch.long)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 32  \n",
    "\n",
    "# Create the DataLoader.\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION DONE.\n"
     ]
    }
   ],
   "source": [
    "# Put model in evaluation mode\n",
    "my_trained_model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "my_predictions, my_true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = my_trained_model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    my_predictions.append(logits)\n",
    "    my_true_labels.append(label_ids)\n",
    "\n",
    "print('PREDICTION DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Matthews Corr. Coef. for each batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    }
   ],
   "source": [
    "# calculating matthews_corrcoef\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "bret_matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(my_true_labels)):\n",
    "\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(my_predictions[i], axis=1).flatten()\n",
    "\n",
    "    # Calculate and store the coef for this batch.  \n",
    "    matthews = matthews_corrcoef(my_true_labels[i], pred_labels_i)                \n",
    "    bret_matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6695340634119862,\n",
       " 0.632183908045977,\n",
       " 1.0,\n",
       " 0.35986374603287324,\n",
       " 1.0,\n",
       " 0.8027729719194864,\n",
       " 0.762962962962963,\n",
       " 0.632183908045977,\n",
       " 0.8027729719194864,\n",
       " 0.6956083436402525,\n",
       " -0.03225806451612903,\n",
       " 1.0,\n",
       " 0.9078412990032037,\n",
       " 0.8027729719194864,\n",
       " 0.0,\n",
       " 0.8027729719194864,\n",
       " 0.8783100656536799,\n",
       " 0.8783100656536799,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.8509629433967631,\n",
       " 0.6956083436402525,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.8783100656536799,\n",
       " 0.8459051693633014,\n",
       " 0.6,\n",
       " 0.5447047794019222,\n",
       " 0.8027729719194864,\n",
       " 0.5584155773160767,\n",
       " 0.8509629433967631,\n",
       " 1.0,\n",
       " 0.8783100656536799,\n",
       " 0.6753002216523571,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.04637388957601683,\n",
       " 0.8027729719194864,\n",
       " 0.8027729719194864,\n",
       " 0.632183908045977,\n",
       " -0.03225806451612903,\n",
       " 0.8027729719194864,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.8958064164776167,\n",
       " 0.632183908045977,\n",
       " 1.0,\n",
       " 0.4666666666666667,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.8027729719194864,\n",
       " 0.6695340634119862,\n",
       " 0.6956083436402525,\n",
       " 0.7142857142857143,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.52678658400752,\n",
       " 1.0,\n",
       " 0.5584155773160767,\n",
       " 0.0,\n",
       " 0.7867957924694432]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bret_matthews_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC: 0.738\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "my_flat_predictions = [item for sublist in my_predictions for item in sublist]\n",
    "my_flat_predictions = np.argmax(my_flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "my_flat_true_labels = [item for sublist in my_true_labels for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(my_flat_true_labels, my_flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9948287e-01, 5.1712402e-04],\n",
       "       [9.9997997e-01, 2.0009060e-05],\n",
       "       [9.9949026e-01, 5.0969986e-04],\n",
       "       ...,\n",
       "       [9.9961901e-01, 3.8091862e-04],\n",
       "       [9.9950075e-01, 4.9917988e-04],\n",
       "       [4.2033759e-03, 9.9579668e-01]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the prediction result of the best LSTM\n",
    "my_lstm_1.predict(np.array(test_inputs), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pred = my_lstm_1.predict_classes(np.array(test_inputs),batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6716558797709505"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "lstm_mcc = matthews_corrcoef(np.array(test_labels), lstm_pred)\n",
    "lstm_mcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the comparison above, we found that the Bret model does a little better on the MCC score on the test set. However, considering the much longer time it takes to train, I think using LSTM might be sufficient for my model.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings, Context Words\n",
    "\n",
    "We saw how a bootstrapped BERT model performed so much better than a model trained from scatch. Because BERT's method of capturing context is bidirectional, meaning that words can now have different word embedding values based on their location within a sentence. Let us use the same BERT model to capture sentence and word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through the sentence format for the BERT model, as well as how our vocabulary looks like. Note that you have to use the BERT tokenizer to use the BERT model because of the similar vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTS model uses a WordPiece technique to do its tokenizing, as described in the paper. That's why the word embedding is split up the way it is.\n",
    "A quick peek at what the voabulary looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['peninsula',\n",
       " 'adults',\n",
       " 'novels',\n",
       " 'emerged',\n",
       " 'vienna',\n",
       " 'metro',\n",
       " 'debuted',\n",
       " 'shoes',\n",
       " 'tamil',\n",
       " 'songwriter',\n",
       " 'meets',\n",
       " 'prove',\n",
       " 'beating',\n",
       " 'instance',\n",
       " 'heaven',\n",
       " 'scared',\n",
       " 'sending',\n",
       " 'marks',\n",
       " 'artistic',\n",
       " 'passage',\n",
       " 'superior',\n",
       " '03',\n",
       " 'significantly',\n",
       " 'shopping',\n",
       " '##tive',\n",
       " 'retained',\n",
       " '##izing',\n",
       " 'malaysia',\n",
       " 'technique',\n",
       " 'cheeks']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[6000:6030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indices.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment ID\n",
    "\n",
    "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
    "\n",
    "If you want to process two sentences, assign each word in the first sentence plus the ‘[SEP]’ token a 0, and all tokens of the second sentence a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print(segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did for classification, we now convert these segments to tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer is the hidden state layer, and this is what we pick up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model_embedding.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_embedding(tokens_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0][0][0]), len(output[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "This kind of forward pass returns us the last layer of the net, which we will use to make our vectors. The first object returned contains the batch number, followed by each of the tokens and their vector values. The second object contains a vector value, which I suspect is the sentence vector of the tokens. \n",
    "\n",
    "The first index is the batch size, and our batch size is 1, so we just choose the 0th index and work with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings, sentence_embedding = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
       "        [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
       "        [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
       "        ...,\n",
       "        [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
       "        [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
       "        [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a quick look at the range of values for a given layer and token.\n",
    "\n",
    "You’ll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vec = word_embeddings[0][0]\n",
    "vec = vec.detach().numpy()\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values are grouped by layer - we can use the permute function to make it grouped by each individual token instead. Let us look at what the later looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "\n",
    "So each of those tokens have embedding values - let us try and compare them with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vecs = []\n",
    "# For each token in the sentence...\n",
    "for embedding in word_embeddings[0]:\n",
    "    cat_vec = embedding.detach().numpy()\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs.append(cat_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
       "        [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
       "        [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
       "        ...,\n",
       "        [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
       "        [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
       "        [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method to create the vectors is to sum the last four layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Vector\n",
    "\n",
    "To get a single vector for our entire sentence we have multiple application-dependent strategies - we could just average all the tokens in our sentence. We can also use this oppurtunity to see if the second vector returned is a sentence vector too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 768)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_embedding_0), len(sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the power of these vectors is how they are context dependant - our sentence had multiple uses of the word bank. Let us see the index and the word of the sentence and check the context accordingly. We'll then print the simlarity values for the similar and different meanings and see how it turns out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "    print(i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    [ 0.9001056  -0.53804165 -0.16690847  0.22416186  0.6896585 ]\n",
      "bank robber   [ 0.7977126  -0.52172744 -0.1983698   0.18898535  0.59409326]\n",
      "river bank    [ 0.29608926 -0.28563383 -0.03818326  0.16736214  0.7712624 ]\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs[19][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.95\n",
      "Vector similarity for *different* meanings:  0.70\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs[10], token_vecs[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense! Let us see if the mean value of all the tokens and what we think is the sentence vector is the same thing, by checking their cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008313185535371304"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is good - it seems it is indeed the sentence vector, so we can now write two functions which calculate the word and sentence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(text, word_id, model, tokenizer):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
    "    vector = word_embeddings[0][word_id].detach().numpy()\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_10 = word_vector(text, 6, model_embedding, tokenizer)\n",
    "word_6 = word_vector(text, 10, model_embedding, tokenizer)\n",
    "word_19 = word_vector(text, 19, model_embedding, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    word_embeddings, sentence_embeddings = model(tokens_tensor)\n",
    "    token_vecs = []\n",
    "    \n",
    "    for embedding in word_embeddings[0]:\n",
    "        cat_vec = embedding.detach().numpy()\n",
    "        token_vecs.append(cat_vec)\n",
    "        \n",
    "    if method == \"average\":\n",
    "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
    "    if method == \"model\":\n",
    "        sentence_embedding = sentence_embeddings\n",
    "    # do something\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_vec_0 = sentence_vector(text, model_embedding, tokenizer)\n",
    "sen_vec_1 = sentence_vector(text, model_embedding, tokenizer, method=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity metrics\n",
    "It is worth noting that word-level similarity comparisons are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This enables direct sensitivity to polysemy so that, e.g., your representation encodes river “bank” and not a financial institution “bank”. Nevertheless, it makes direct word-to-word similarity comparisons less valuable. For sentence embeddings, however, similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs as some similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n",
    "\n",
    "### Using the Vectors\n",
    "\n",
    "Without fine-tuning, BERT features may be less useful than plain GloVe or word2vec.\n",
    "They start to be interesting when you fine-tune a classifier on top of BERT. \n",
    "\n",
    "### Using Transformers Pipelines\n",
    "\n",
    "The context vectors make the other pipeline functions which transformers has built in a lot more powerful. \n",
    "\n",
    "### NOTE\n",
    "The pipeline functionality in transformers is currently being worked on and might be broken, so it is an optional part of the exercise. Do try to uncomment the lines of code and try to see if it works, though! If you have managed to get transformers v2.5.1 installed, it will work - I managed to get it to work sometimes, it can be annoying to get it to work but when it works it works well.\n",
    "\n",
    "Consider the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0495ab820a244622bda1d84cbb775360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't reach server at 'https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-finetuned-sst-2-english-modelcard.json' to download model card file.\n",
      "Creating an empty model card.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5613f8bb2c49b88402d5912648cb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267844284.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for sentiment-analysis\n",
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997735}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"This BERT model is so good at classifiying sentiment, I love it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a strong positive sentiment, which we'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997195}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"I'm so sad that I have to spend this weekend just doing HW and readings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative label, bingo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91215cef5e4b45a19a72f5bb1ba67930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=939.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7ebd8be4b34dfdb58292290f6751ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c322339e4e0640b18c4264c0a4ce412d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8af4e4437c473bac877527a000d4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1031.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a970b191ec75453ea63602a50ddd64d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=260793700.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for question-answeringn\n",
    "nlp_question = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|███████████████████████████████████████████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "add example index and unique id: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 1005.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9860030258239121,\n",
       " 'start': 34,\n",
       " 'end': 64,\n",
       " 'answer': 'analysing complex textual data'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_question({\n",
    "    'question': 'What is my favorite thing to do on weekends ?',\n",
    "    'context': 'There is nothing I like more than analysing complex textual data all weekend '\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also great at question-answering tasks!\n",
    "We can also extract features, as we manually did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187012833b9a494b869abf31e5ae78d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7bc3a542fe43298ead593fd464afb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263273408.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_feature = pipeline('feature-extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = nlp_feature(\"Just sitting here exploring data all day long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huggingface/transformers repository lists the other pipeline functions, such as ner extraction, sequence classification, and masking. You are encouraged to explore them. \n",
    "https://github.com/huggingface/transformers#quick-tour-of-pipelines\n",
    "\n",
    "## <span style=\"color:red\">*Exercise 2*</span>\n",
    "\n",
    "<span style=\"color:red\">In the cells immediately following, use the pipeline functions or the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. You have used similar, but often weaker versions in previous weeks. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, I would replicate the exercise I have done in HW 4 to show the difference of using Bret. In that problem set, I measured the similarity between 4 corpora using divergence metrics like KL,KS distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpora = [my_df['comment_text'][:20], my_df['comment_text'][20:40],my_df['comment_text'][40:60], my_df['comment_text'][60:80]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
    "my_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpora_vector(corpora, model, tokenizer, method=\"average\"):\n",
    "    sen_vecs = []\n",
    "    for sen in corpora:\n",
    "        try:\n",
    "            # some sentences are too long, so we need to skip them\n",
    "            sen_vecs.append(sentence_vector(sen,model,tokenizer,method))\n",
    "        except:\n",
    "            continue\n",
    "    return np.mean(sen_vecs,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 6.89556152e-02,  6.48047701e-02,  3.06627899e-01, -1.49085477e-01,\n",
       "         2.31523979e-02, -2.64257669e-01,  1.21948957e-01,  6.37152493e-01,\n",
       "        -7.83731788e-02, -1.93867400e-01,  1.66857287e-01, -4.17892694e-01,\n",
       "        -1.12223201e-01,  4.20465708e-01, -3.48276526e-01,  4.42160964e-01,\n",
       "        -7.49179795e-02,  6.82062376e-03, -1.38003647e-01,  2.79126108e-01,\n",
       "         3.60785037e-01, -5.54934749e-03, -5.66711426e-02,  2.42536768e-01,\n",
       "         3.51278484e-01,  2.63111908e-02,  3.78555097e-02,  4.11726423e-02,\n",
       "        -2.26569608e-01, -1.38561368e-01,  3.66291881e-01,  1.31761685e-01,\n",
       "        -9.14688706e-02, -8.61869194e-03, -1.02674533e-02, -1.47656575e-01,\n",
       "        -3.58273238e-02,  5.54630756e-02, -1.77306965e-01,  2.87364662e-01,\n",
       "        -4.97178286e-01, -2.55593777e-01,  2.00743377e-02,  5.33077009e-02,\n",
       "        -2.52298117e-01, -4.01945710e-01,  2.82840312e-01, -3.32656838e-02,\n",
       "         1.11667372e-01, -1.52898207e-01, -2.07322896e-01,  1.57364637e-01,\n",
       "        -2.37690747e-01, -1.31797671e-01,  8.55106637e-02,  5.88048160e-01,\n",
       "        -4.64066900e-02, -5.99771380e-01, -5.15845716e-01, -2.83911955e-02,\n",
       "         2.53167778e-01,  7.10674236e-03,  5.03242016e-02, -2.97594130e-01,\n",
       "         2.57974774e-01,  1.48540452e-01, -6.09645136e-02,  4.14086968e-01,\n",
       "        -6.37428999e-01, -5.81027335e-03, -3.16805691e-01, -4.53479402e-02,\n",
       "        -1.03114292e-01,  1.38315201e-01, -7.26721212e-02,  5.71992025e-02,\n",
       "        -1.37260437e-01,  3.45834404e-01,  6.57700747e-02, -1.05909526e-01,\n",
       "        -2.28504941e-01,  2.73470253e-01, -4.04857129e-01,  4.16148037e-01,\n",
       "         7.67918602e-02,  8.32445920e-02,  1.91166755e-02,  8.67884979e-02,\n",
       "        -3.22327197e-01,  4.48658794e-01,  2.18300059e-01, -3.36313039e-01,\n",
       "         5.64018786e-02, -5.41703701e-02,  1.77586108e-01, -1.13739394e-01,\n",
       "         2.01921895e-01,  1.69023126e-01, -1.98585317e-01,  4.82887864e-01,\n",
       "         1.71101525e-01, -1.31202742e-01,  7.77967945e-02, -1.96446553e-02,\n",
       "        -6.88259378e-02, -1.80142328e-01,  9.31865275e-02,  2.73203224e-01,\n",
       "         1.82847694e-01,  1.29899144e-01,  1.32017553e-01, -1.47023246e-01,\n",
       "        -1.14458650e-01, -2.32765168e-01, -2.40161344e-01,  6.69645146e-02,\n",
       "         1.40184924e-01, -1.95189521e-01,  2.52633225e-02,  1.68099478e-01,\n",
       "        -1.01944305e-01, -1.18187228e-02,  1.19005904e-01,  9.22144413e-01,\n",
       "         8.28547701e-02,  9.93487164e-02, -2.58022934e-01,  2.76975870e-01,\n",
       "        -1.40551060e-01, -1.07264221e-01,  1.47557229e-01,  3.53572100e-01,\n",
       "         1.53550923e-01, -5.16915500e-01, -2.55456835e-01,  1.84797868e-01,\n",
       "         8.62357169e-02, -3.44382882e-01, -4.20554101e-01,  1.01399049e-02,\n",
       "         1.64851490e-02, -1.39093652e-01,  1.83366761e-01,  1.44394904e-01,\n",
       "         4.33826856e-02,  1.47425488e-01, -2.38350317e-01, -1.28971696e-01,\n",
       "         5.74998409e-02,  1.03095993e-01,  3.12088858e-02, -2.01479152e-01,\n",
       "        -1.12279229e-01, -1.30146459e-01, -2.01527268e-01, -3.81674841e-02,\n",
       "        -2.44777381e-01,  1.37867272e-01, -4.30693813e-02,  1.54989377e-01,\n",
       "         4.66368616e-01, -1.92260310e-01, -8.85008350e-02,  3.18681598e-01,\n",
       "         1.59149572e-01, -6.53321818e-02,  5.22382446e-02,  1.93198055e-01,\n",
       "        -7.40392134e-02, -3.98439792e-04,  3.70627944e-03, -1.02797933e-01,\n",
       "         8.61805081e-01, -8.35269988e-02,  1.56316698e-01, -1.18025005e-01,\n",
       "         2.62837976e-01,  1.69430465e-01,  6.06600977e-02,  1.28176495e-01,\n",
       "        -6.81785941e-01,  3.25700194e-01, -2.58931965e-02,  5.99356145e-02,\n",
       "         2.48260751e-01, -3.48438397e-02,  1.07691199e-01, -2.36636341e-01,\n",
       "         1.72299102e-01, -4.85798679e-02, -3.10077488e-01, -1.81925446e-01,\n",
       "        -2.76697278e-01, -8.82599503e-03,  2.81682223e-01, -3.72577995e-01,\n",
       "        -1.34987473e-01, -1.22855626e-01, -3.28838944e-01,  2.71999002e-01,\n",
       "         1.36973113e-01,  2.80979220e-02,  2.03268602e-01,  1.59875154e-01,\n",
       "        -2.98219830e-01,  4.32092324e-03,  1.25061825e-01, -3.95579040e-02,\n",
       "        -1.04649395e-01,  4.16218191e-01, -1.37867644e-01,  4.25757408e-01,\n",
       "        -3.78549891e-03, -2.40601206e-04, -1.06601112e-01, -1.05789844e-02,\n",
       "         4.28116620e-02, -1.48611024e-01,  7.91376233e-02,  3.26805823e-02,\n",
       "        -1.14031089e-02,  2.78958920e-02, -4.79526907e-01,  3.64744753e-01,\n",
       "        -4.88280170e-02,  5.70343792e-01, -5.22909649e-02, -4.47307497e-01,\n",
       "         1.07241251e-01,  3.04201305e-01, -3.26490179e-02, -1.87928811e-01,\n",
       "         6.26584351e-01,  5.64255007e-02, -1.64633885e-01,  1.20677821e-01,\n",
       "        -1.06508076e-01, -2.28043661e-01,  1.81736410e-01, -2.94738203e-01,\n",
       "        -2.40769431e-01,  2.82654345e-01,  3.56185138e-01,  2.18930677e-01,\n",
       "         6.51346222e-02,  8.07147697e-02, -3.00702363e-01, -1.51651755e-01,\n",
       "        -1.56470969e-01, -1.21239208e-01, -2.98367441e-01, -3.69182348e-01,\n",
       "         2.10131984e-02, -5.98838329e-01, -1.15982242e-01, -1.77384689e-01,\n",
       "        -2.48793378e-01, -2.24891439e-01, -1.56030461e-01,  7.04346457e-03,\n",
       "         2.91477203e-01,  1.62622377e-01,  1.35660404e-02,  1.54133812e-01,\n",
       "        -4.40498918e-01, -3.90719950e-01,  1.36871403e-02,  3.16068351e-01,\n",
       "         1.91779733e-01,  3.44648622e-02,  2.37759948e-02, -8.09421837e-02,\n",
       "        -5.61395399e-02,  4.87873077e-01, -2.06014901e-01, -8.24623033e-02,\n",
       "         2.16644734e-01,  1.63215190e-01,  1.36041185e-02, -2.71746784e-01,\n",
       "         1.42413437e-01,  4.34422702e-01, -2.33204693e-01,  8.79550949e-02,\n",
       "        -1.21871391e-02, -2.31871963e-01,  3.94373834e-02,  3.18812966e-01,\n",
       "        -3.17461550e-01, -3.64935458e-01, -7.33122230e-02,  9.83914062e-02,\n",
       "        -3.58166546e-01, -1.21704601e-01,  3.54055375e-01, -8.38456955e-03,\n",
       "         1.20338827e-01,  1.55277640e-01,  2.21842334e-01, -3.31111997e-01,\n",
       "        -1.92444697e-01,  6.07180037e-02,  2.35630617e-01,  3.32519621e-01,\n",
       "        -3.35597187e-01,  1.37655541e-01, -1.09802023e-01, -3.66915405e-01,\n",
       "        -3.71066880e+00, -3.53261232e-02,  1.62965015e-01, -2.88385153e-01,\n",
       "         3.52513343e-01, -1.20501131e-01,  8.49068537e-02, -2.27419198e-01,\n",
       "        -4.70350981e-01, -1.42782748e-01,  4.94220667e-02, -2.18030527e-01,\n",
       "         5.03868639e-01,  1.45958677e-01,  2.55111039e-01,  2.32711315e-01,\n",
       "         1.78899005e-01, -2.57067859e-01, -3.06977570e-01,  4.81984645e-01,\n",
       "        -8.46750066e-02, -1.89659506e-01,  3.03974241e-01, -1.99453950e-01,\n",
       "         4.19460118e-01,  3.85071456e-01, -1.78774446e-01,  9.17752013e-02,\n",
       "        -1.80639133e-01, -2.03173608e-01,  3.87416519e-02, -1.52933225e-01,\n",
       "         8.88056234e-02,  1.07252136e-01,  3.55837680e-02, -4.60045896e-02,\n",
       "        -1.08236680e-02, -2.94723868e-01, -4.63848375e-02, -1.69022128e-01,\n",
       "        -1.11458816e-01, -5.27581334e-01, -4.10442911e-02, -1.81851819e-01,\n",
       "         7.15554655e-01, -2.50572324e-01, -1.43435210e-01, -3.42400134e-01,\n",
       "        -5.47243617e-02,  2.45449394e-01,  1.12368085e-04, -2.03740612e-01,\n",
       "        -2.17966914e-01, -1.53884813e-01, -3.16462815e-02,  1.28014684e-01,\n",
       "         3.98928344e-01,  5.49937487e-01, -2.48497382e-01, -5.69190323e-01,\n",
       "        -2.93091405e-02, -1.41939640e-01, -5.61088204e-01, -9.37712416e-02,\n",
       "        -5.95188327e-02, -2.44958654e-01, -2.95561522e-01, -2.11474240e-01,\n",
       "         1.23484038e-01,  5.85143566e-02, -4.60873879e-02,  3.46761644e-01,\n",
       "        -3.56988579e-01, -7.81409979e-01, -1.41129851e-01,  1.38604015e-01,\n",
       "        -1.33963987e-01, -7.18780234e-03,  1.05525509e-01, -8.73484537e-02,\n",
       "        -2.63164282e-01, -1.91490173e-01, -1.27140403e-01, -1.40331537e-01,\n",
       "        -2.87539393e-01, -3.60287189e-01,  1.79399818e-01, -2.63655066e-01,\n",
       "        -3.51823151e-01, -4.99900669e-01,  2.50122428e-01,  1.11075804e-01,\n",
       "         5.03065102e-02,  1.97079346e-01,  2.57680684e-01,  6.74044564e-02,\n",
       "         4.40093696e-01, -3.33086997e-01,  2.40537837e-01, -1.85445935e-01,\n",
       "         2.31995508e-01,  4.45864052e-02,  6.50588036e-01, -2.56435603e-01,\n",
       "         2.82152891e-01,  2.61624232e-02, -3.40528816e-01,  1.01562589e-01,\n",
       "        -2.52233110e-02,  7.05244467e-02,  3.93382430e-01, -5.80580235e-01,\n",
       "         4.53322649e-01, -1.75735652e-01, -3.77600580e-01, -2.74888635e-01,\n",
       "         3.89686167e-01,  3.70884269e-01, -1.10964172e-01, -1.84925482e-01,\n",
       "        -6.32478595e-02,  4.66027766e-01, -5.41578889e-01, -5.19664466e-01,\n",
       "        -6.21588826e-01, -5.30168191e-02, -3.42844576e-01,  5.84682412e-02,\n",
       "        -2.39237234e-01,  1.77084848e-01, -1.58143923e-01, -2.58000165e-01,\n",
       "        -1.20577976e-01,  2.10719511e-01,  9.87215191e-02, -5.39750978e-02,\n",
       "        -3.44247445e-02, -4.71203297e-01, -7.71843791e-02,  4.83992994e-02,\n",
       "        -8.88268743e-03,  1.31341055e-01, -5.05194589e-02,  5.23507595e-02,\n",
       "        -2.82806605e-02,  2.62102664e-01, -1.80096895e-01, -1.52012810e-01,\n",
       "        -6.35508671e-02,  1.78997949e-01, -2.92406648e-01, -4.55335706e-01,\n",
       "         2.03647427e-02, -4.17319000e-01,  2.14544177e-01,  4.50329669e-02,\n",
       "         2.41613582e-01, -9.16780084e-02, -5.52616417e-02, -3.72476906e-01,\n",
       "        -9.78470519e-02,  9.25279185e-02,  1.67357519e-01,  3.44078451e-01,\n",
       "        -9.51112434e-02,  3.88562083e-01, -1.91643205e-03, -1.40728265e-01,\n",
       "        -5.32041490e-02,  3.66423652e-02, -3.13098788e-01,  4.66904156e-02,\n",
       "        -1.93724826e-01, -2.01444134e-01, -6.50075227e-02,  5.38738608e-01,\n",
       "         2.07736604e-02, -1.63920715e-01, -5.25233001e-02,  2.84492701e-01,\n",
       "        -5.06373942e-02, -1.57535836e-01, -9.55004469e-02,  8.62679631e-03,\n",
       "         3.92733723e-01,  1.80623114e-01,  1.54902071e-01, -2.87451327e-01,\n",
       "         1.93193316e-01,  1.17515363e-01, -4.18306351e-01,  5.24559200e-01,\n",
       "        -2.00599149e-01, -5.10350019e-02, -3.21432739e-01, -1.46702975e-01,\n",
       "         4.88180667e-01, -3.09852213e-01,  1.54584676e-01,  2.50978857e-01,\n",
       "         3.83724302e-01,  1.51038561e-02, -7.96181336e-02,  2.42881194e-01,\n",
       "        -3.87801789e-02, -1.83676779e-01,  1.23125717e-01, -1.11889951e-02,\n",
       "        -2.36194134e-02,  1.33764237e-01, -1.99660122e-01, -3.81459832e-01,\n",
       "        -3.70514005e-01, -1.74008712e-01,  1.50599435e-01, -3.92946005e-01,\n",
       "         1.06052525e-01, -1.11430697e-01, -2.96566784e-01,  1.81902781e-01,\n",
       "        -1.32857367e-01, -7.88252503e-02,  7.17344433e-02,  1.68900222e-01,\n",
       "        -4.34332043e-01, -2.75774837e-01,  7.54712150e-02,  6.91204444e-02,\n",
       "        -1.81714177e-01, -1.14502214e-01,  8.20803717e-02, -5.88431478e-01,\n",
       "         2.61896133e-01,  2.38586828e-01, -1.50561929e-01,  2.70936102e-01,\n",
       "         1.02636991e-02, -1.95330814e-01,  1.14685796e-01, -1.21160433e-01,\n",
       "        -3.67161669e-02,  3.57499383e-02, -3.78156602e-02, -4.82521206e-01,\n",
       "         7.63242617e-02,  1.21182017e-01, -7.06282854e-02,  5.23357630e-01,\n",
       "        -2.81066269e-01,  3.41590732e-01,  1.52376369e-01,  1.05118945e-01,\n",
       "        -1.59561887e-01, -3.35033655e-01,  1.36316091e-01, -5.85786343e-01,\n",
       "        -4.68120009e-01,  9.95666385e-02, -1.79422393e-01,  8.45078453e-02,\n",
       "        -4.44236696e-02,  2.90096682e-02,  4.37386474e-03,  1.56971484e-01,\n",
       "         6.27386644e-02,  1.33616462e-01,  2.12788761e-01,  2.50242114e-01,\n",
       "        -5.66130579e-02, -3.50717962e-01, -8.03702474e-02, -1.03459358e-01,\n",
       "         1.65480897e-01, -1.67885736e-01, -6.77084550e-02,  7.71401972e-02,\n",
       "        -2.69334555e-01, -4.16067660e-01,  5.39116323e-01, -3.18714827e-01,\n",
       "        -3.11336834e-02,  1.02422513e-01, -3.25726084e-02, -1.31673321e-01,\n",
       "         2.06709862e-01, -2.14257941e-01, -7.27315843e-02,  2.50024229e-01,\n",
       "        -2.12087378e-01, -7.22972602e-02,  2.45610893e-01,  2.63919890e-01,\n",
       "        -9.21549574e-02,  2.60156486e-02,  3.10426708e-02,  1.95271984e-01,\n",
       "         4.77713287e-01, -4.42243293e-02,  1.56192541e-01,  1.88318372e-01,\n",
       "        -1.00636981e-01, -8.88501480e-02,  1.97562099e-01,  2.36808896e-01,\n",
       "        -1.05135217e-01,  1.94850996e-01,  7.01780319e-02, -2.96172231e-01,\n",
       "         2.48104826e-01,  2.71747500e-01, -2.22463205e-01, -3.62163037e-01,\n",
       "         6.55082524e-01,  3.23654681e-01, -2.33166039e-01, -3.80783111e-01,\n",
       "         1.21756263e-01, -1.61053672e-01, -1.56801477e-01,  1.25343855e-02,\n",
       "         6.59972578e-02, -1.12535805e-01,  6.53118372e-01,  7.82556012e-02,\n",
       "         2.02224866e-01,  2.25507140e-01, -5.05206943e-01, -8.81772861e-02,\n",
       "         1.69815477e-02,  2.66024500e-01, -6.66659921e-02,  2.22979501e-01,\n",
       "        -2.78816938e-01,  4.60038096e-01, -1.29718482e-01, -1.52049109e-01,\n",
       "        -9.51716676e-03,  4.24490161e-02,  1.46322891e-01,  4.63099368e-02,\n",
       "        -8.16316381e-02,  1.77695736e-01,  3.59323233e-01,  5.31044781e-01,\n",
       "         2.62983084e-01,  1.37413234e-01,  8.19283649e-02, -1.89448483e-02,\n",
       "         7.84229040e-01,  4.22905415e-01,  1.23351865e-01,  4.72612888e-01,\n",
       "        -1.36722714e-01, -2.26416945e-01,  1.12454034e-01,  2.61150271e-01,\n",
       "         2.03876615e-01,  2.57464319e-01, -1.32660165e-01,  4.00673956e-01,\n",
       "         1.46906316e-01, -5.30816875e-02,  4.88987975e-02, -3.41354102e-01,\n",
       "        -2.09134892e-01,  2.04867914e-01,  1.50451839e-01, -2.46578872e-01,\n",
       "        -2.82944739e-01, -3.69620174e-02, -4.49575521e-02,  7.65790343e-02,\n",
       "        -2.72200912e-01, -7.71720409e-02, -4.32941824e-01,  3.03964764e-01,\n",
       "         1.31577536e-01, -1.03044711e-01, -1.90463096e-01,  4.40945514e-02,\n",
       "        -1.80768932e-03, -1.43451709e-02, -4.48932290e-01, -1.35742232e-01,\n",
       "        -1.04050986e-01, -6.42733634e-01, -1.37557462e-01,  8.54731053e-02,\n",
       "        -1.31190330e-01, -3.84662509e-01, -2.29701083e-02, -1.36579601e-02,\n",
       "        -3.16831805e-02, -2.02547893e-01,  1.26276687e-02, -2.73480326e-01,\n",
       "         4.52211022e-01,  2.81971663e-01,  3.63127202e-01,  1.65781140e-01,\n",
       "         1.45654604e-01, -2.14500859e-01, -7.11408779e-02,  1.26483798e-01,\n",
       "         2.76084572e-01,  1.07028820e-01, -6.99167997e-02,  2.74057209e-01,\n",
       "        -3.42956394e-01,  5.75247407e-03, -1.40248194e-01,  5.30519187e-01,\n",
       "        -8.03662658e-01,  2.13732019e-01, -1.05477370e-01, -9.11856592e-02,\n",
       "         9.26093012e-02,  2.20627822e-02, -2.69145787e-01,  2.64261156e-01,\n",
       "        -3.50519717e-01, -2.45356485e-01,  7.44560286e-02,  2.10202530e-01,\n",
       "        -1.65144011e-01, -7.90962726e-02,  4.05706391e-02,  4.16862071e-01,\n",
       "         9.44049750e-03, -1.62489533e-01,  1.01072066e-01,  2.00624123e-01,\n",
       "         6.59779087e-02, -4.96838912e-02, -4.33134250e-02, -1.87463760e-01,\n",
       "        -3.05652142e-01,  4.07226197e-02, -2.48674616e-01,  1.41998939e-02,\n",
       "         2.87248790e-01,  1.02974870e-03,  4.60062288e-02, -7.27027059e-02,\n",
       "         2.70899355e-01, -7.83571228e-02,  4.91135940e-02, -1.04631118e-01,\n",
       "        -7.59729892e-02, -4.03185129e-01, -1.65660027e-02,  3.95502523e-02,\n",
       "         9.95110646e-02, -1.54741883e-01,  6.87194467e-02, -1.89462945e-01,\n",
       "        -8.89361724e-02, -2.32491016e-01,  9.47594866e-02, -4.85727638e-02],\n",
       "       dtype=float32),\n",
       " array([ 4.46958914e-02,  4.15166281e-02,  2.39414930e-01, -6.95746318e-02,\n",
       "         1.02191113e-01, -2.08287627e-01,  3.70804332e-02,  5.55702448e-01,\n",
       "        -4.23586592e-02, -1.56991631e-01,  1.45915523e-01, -3.93302351e-01,\n",
       "        -1.28658757e-01,  4.02462870e-01, -2.90633559e-01,  3.90930116e-01,\n",
       "        -4.21498008e-02, -6.13341760e-03, -1.36064976e-01,  2.71026701e-01,\n",
       "         3.25351536e-01, -2.24594884e-02, -3.62623259e-02,  3.68968427e-01,\n",
       "         3.34426582e-01, -2.50891093e-02,  5.96825704e-02,  1.99279511e-05,\n",
       "        -2.32148647e-01, -6.71253949e-02,  3.77908826e-01,  1.05845250e-01,\n",
       "        -9.87905711e-02, -1.03706166e-01, -1.11499205e-01, -1.19260237e-01,\n",
       "        -9.41766053e-02, -5.49589507e-02, -8.75248760e-02,  2.12920234e-01,\n",
       "        -4.80563730e-01, -2.27487490e-01,  1.25086783e-02,  1.15468346e-01,\n",
       "        -2.25438192e-01, -2.99974531e-01,  2.71234065e-01, -1.01640463e-01,\n",
       "         1.16856955e-01, -1.25445694e-01, -3.84726971e-01,  1.93203345e-01,\n",
       "        -2.22858116e-01,  6.17057085e-04,  1.12091005e-01,  6.21195078e-01,\n",
       "        -1.04564764e-01, -5.40731788e-01, -5.12041330e-01, -1.23274148e-01,\n",
       "         2.91207373e-01, -1.34669915e-01,  5.95525838e-03, -3.35248291e-01,\n",
       "         2.42754057e-01,  1.44412592e-01,  4.52574566e-02,  3.73195112e-01,\n",
       "        -6.27349079e-01,  4.69019450e-03, -2.14568883e-01, -7.86155462e-02,\n",
       "        -1.30976006e-01,  1.23155318e-01, -1.10478878e-01,  6.46079844e-03,\n",
       "        -4.63450216e-02,  3.69476616e-01,  7.98767358e-02, -1.68918356e-01,\n",
       "        -2.89962441e-01,  3.35575283e-01, -3.73009861e-01,  3.59426260e-01,\n",
       "         8.60866979e-02,  1.13707855e-02, -5.34258895e-02,  7.29330182e-02,\n",
       "        -3.40824604e-01,  4.71360505e-01,  1.67756289e-01, -2.96336621e-01,\n",
       "        -5.98371029e-04, -2.16188096e-03,  2.57447034e-01, -1.42806947e-01,\n",
       "         1.57188922e-01,  1.26020923e-01, -1.68400943e-01,  4.57047701e-01,\n",
       "         1.36007622e-01, -1.29917234e-01,  1.11244857e-01, -8.38588178e-02,\n",
       "        -3.82076614e-02, -1.82694480e-01,  1.54235244e-01,  3.07918191e-01,\n",
       "         1.66079730e-01,  1.79236054e-01,  1.60508484e-01, -6.67428002e-02,\n",
       "        -1.36681482e-01, -2.33267263e-01, -1.98030606e-01,  2.81435046e-02,\n",
       "         1.09336458e-01, -1.22789100e-01,  2.85755508e-02,  1.26380280e-01,\n",
       "        -1.52242258e-01, -6.50731474e-02,  1.10860363e-01,  8.01269710e-01,\n",
       "         9.18590575e-02,  1.15725294e-01, -2.16761306e-01,  2.10343450e-01,\n",
       "        -1.62383229e-01, -1.36961639e-01,  1.94213420e-01,  3.59391838e-01,\n",
       "         1.46284640e-01, -4.62577045e-01, -2.46868521e-01,  1.65410444e-01,\n",
       "        -2.88292468e-02, -3.31791580e-01, -4.07111257e-01,  6.33208379e-02,\n",
       "         6.72966316e-02, -3.83531861e-02,  2.42537886e-01,  9.02707875e-02,\n",
       "         4.26264927e-02,  1.04719207e-01, -1.97680041e-01, -1.35478288e-01,\n",
       "        -5.61177619e-02,  1.32060900e-01,  2.68707164e-02, -1.52825892e-01,\n",
       "        -5.11092320e-02, -1.42578870e-01, -1.58666402e-01, -5.38074598e-02,\n",
       "        -2.26062804e-01,  2.20868498e-01, -4.81217131e-02,  1.39736980e-01,\n",
       "         4.72626388e-01, -4.17213663e-02, -4.44533415e-02,  2.80645996e-01,\n",
       "         1.79449871e-01, -5.90083376e-02, -4.27064765e-03,  2.08873779e-01,\n",
       "        -1.77269757e-01, -1.62146089e-03, -9.11455005e-02, -1.49332955e-01,\n",
       "         8.47474396e-01, -8.13026279e-02,  4.47848588e-02, -7.22580701e-02,\n",
       "         2.58872926e-01,  1.32100508e-01,  1.81276381e-01,  5.46247959e-02,\n",
       "        -6.34782672e-01,  2.99069405e-01,  2.00136714e-02,  4.23638113e-02,\n",
       "         2.02470332e-01, -1.16528139e-01,  1.08270064e-01, -1.76881000e-01,\n",
       "         1.63202316e-01, -3.16321477e-02, -3.27844560e-01, -2.78698146e-01,\n",
       "        -2.79331744e-01, -3.36425975e-02,  2.83389807e-01, -3.34622085e-01,\n",
       "        -1.79655224e-01, -5.85509650e-02, -3.69787842e-01,  1.68437809e-01,\n",
       "         1.80014998e-01,  5.03880493e-02,  2.27941155e-01,  1.75554782e-01,\n",
       "        -2.07471251e-01, -4.75512147e-02,  1.68195546e-01, -4.03768942e-02,\n",
       "        -5.62192798e-02,  3.62974524e-01, -1.57739252e-01,  4.58553731e-01,\n",
       "        -6.86521083e-02,  3.90047356e-02, -1.57354146e-01,  7.12656900e-02,\n",
       "         5.44082746e-02, -9.39096957e-02,  1.35809273e-01, -1.32681394e-03,\n",
       "         6.33315742e-02,  3.54613662e-02, -4.17724997e-01,  3.36083859e-01,\n",
       "        -1.35737704e-02,  5.65199614e-01, -5.37072010e-02, -5.04037797e-01,\n",
       "         2.83951521e-01,  4.08488601e-01, -6.83886483e-02, -2.42986485e-01,\n",
       "         7.04097390e-01,  7.26748630e-02, -8.62959325e-02,  2.99894549e-02,\n",
       "        -1.88064188e-01, -2.06245273e-01,  9.73474458e-02, -1.52736187e-01,\n",
       "        -2.12180972e-01,  3.68355572e-01,  2.91097701e-01,  2.10896060e-01,\n",
       "         1.05570398e-01, -1.41120283e-02, -1.91205457e-01, -1.14567555e-01,\n",
       "        -1.65187001e-01, -1.37607843e-01, -4.00971323e-01, -3.42718780e-01,\n",
       "         3.21964314e-03, -5.36219597e-01, -1.30184457e-01, -2.36372188e-01,\n",
       "        -2.32588142e-01, -2.69027799e-01, -2.25064047e-02,  8.44369829e-02,\n",
       "         3.03352982e-01,  1.47159383e-01, -5.28838336e-02,  1.01797402e-01,\n",
       "        -4.56112683e-01, -4.31030273e-01,  1.57528068e-03,  2.89850652e-01,\n",
       "         2.18358129e-01,  1.84673086e-01, -4.78120819e-02, -6.83606789e-02,\n",
       "        -2.15912368e-02,  4.90187347e-01, -2.30040103e-01, -4.55368385e-02,\n",
       "         1.92628652e-01,  1.22870132e-01,  7.58402124e-02, -1.58294991e-01,\n",
       "         1.65792450e-01,  4.69707906e-01, -1.83693781e-01,  7.70674422e-02,\n",
       "        -3.71439904e-02, -2.99936771e-01,  1.59269005e-01,  2.54999250e-01,\n",
       "        -2.54025847e-01, -3.58801752e-01, -1.24822035e-01,  1.59432799e-01,\n",
       "        -2.41554573e-01, -2.36699134e-01,  3.85960281e-01,  4.95279655e-02,\n",
       "         1.85539812e-01,  8.08793828e-02,  1.11432396e-01, -2.81583309e-01,\n",
       "        -2.28219509e-01, -8.06208700e-03,  2.38300636e-01,  2.46035725e-01,\n",
       "        -1.81884050e-01,  1.82781532e-01, -5.86361811e-02, -3.49349290e-01,\n",
       "        -3.80607843e+00, -3.17949019e-02,  1.66604310e-01, -3.26676339e-01,\n",
       "         1.98966265e-01, -1.12073734e-01, -4.13381681e-02, -2.25159496e-01,\n",
       "        -4.19872940e-01, -1.71875581e-01,  1.94843002e-02, -2.19812304e-01,\n",
       "         3.50113213e-01,  2.60593593e-01,  2.40990400e-01,  1.02928542e-01,\n",
       "         1.04205944e-01, -1.43838048e-01, -2.30760261e-01,  5.14237761e-01,\n",
       "        -1.35399848e-01, -2.41114900e-01,  2.55004704e-01, -2.03066975e-01,\n",
       "         3.20307434e-01,  3.60539019e-01, -1.79961413e-01, -2.75848415e-02,\n",
       "        -2.47016221e-01, -2.22383976e-01,  7.90711492e-02, -2.29681879e-01,\n",
       "         2.90279323e-03,  1.93840221e-01,  5.02265766e-02, -3.07152350e-03,\n",
       "        -3.90030593e-02, -3.62270296e-01, -7.13964254e-02, -1.89494655e-01,\n",
       "        -1.69258729e-01, -6.48153663e-01, -5.35205146e-03, -2.02160507e-01,\n",
       "         7.05423951e-01, -2.93469965e-01, -9.12468880e-02, -2.65769362e-01,\n",
       "         5.53190755e-03,  2.51973569e-01,  6.72306791e-02, -1.05097234e-01,\n",
       "        -1.90966874e-01, -1.16840936e-01, -9.09410566e-02,  3.45350057e-02,\n",
       "         4.19642925e-01,  5.41855454e-01, -2.36479044e-01, -5.01140058e-01,\n",
       "        -2.66690161e-02, -1.48656502e-01, -5.03944635e-01, -1.25157699e-01,\n",
       "        -1.10709608e-01, -2.75309980e-01, -3.59625638e-01, -1.13055959e-01,\n",
       "         1.32163018e-01,  8.85649920e-02, -3.30640003e-02,  3.42156798e-01,\n",
       "        -3.91065001e-01, -6.93531156e-01, -8.94404426e-02, -3.38474512e-02,\n",
       "        -6.37810379e-02, -3.60988975e-02,  9.76988599e-02, -1.67986972e-03,\n",
       "        -2.66128004e-01, -2.43258044e-01, -4.51221764e-02, -1.54235348e-01,\n",
       "        -1.72122002e-01, -3.86604726e-01,  1.02782860e-01, -2.25388527e-01,\n",
       "        -2.97502577e-01, -5.67016721e-01,  2.54717797e-01,  1.66700512e-01,\n",
       "         5.14231026e-02,  1.93087965e-01,  2.34827682e-01,  4.26657535e-02,\n",
       "         3.16191733e-01, -2.58724719e-01,  2.29364842e-01, -1.59299791e-01,\n",
       "         2.70409942e-01,  2.21387781e-02,  6.48034692e-01, -2.23941684e-01,\n",
       "         1.49714455e-01,  2.73029767e-02, -3.41951847e-01,  1.66755736e-01,\n",
       "         3.39672565e-02,  8.81026499e-03,  3.55456918e-01, -5.86928248e-01,\n",
       "         4.38820213e-01, -2.51489758e-01, -3.65284115e-01, -2.22608209e-01,\n",
       "         3.66720229e-01,  3.15926522e-01, -9.72560868e-02, -2.66708136e-01,\n",
       "        -4.19304594e-02,  5.10755301e-01, -3.84643257e-01, -5.28402269e-01,\n",
       "        -5.61087489e-01, -1.44897308e-02, -2.60162771e-01,  7.19708577e-02,\n",
       "        -1.48317054e-01,  1.21165767e-01, -1.85763821e-01, -2.21591502e-01,\n",
       "        -1.76609442e-01,  3.69033277e-01,  8.56631547e-02, -7.11960122e-02,\n",
       "        -1.02499224e-01, -4.16173548e-01,  1.75239407e-02,  1.47686511e-01,\n",
       "        -1.06996652e-02,  1.66159317e-01, -2.23532580e-02, -2.76266374e-02,\n",
       "        -4.89324611e-03,  2.88350165e-01, -6.45621270e-02, -1.13032714e-01,\n",
       "        -1.26678407e-01,  5.98673038e-02, -2.95591980e-01, -3.80778581e-01,\n",
       "         3.33832428e-02, -3.68145883e-01,  2.07427338e-01,  9.32250842e-02,\n",
       "         2.96708912e-01, -2.29230169e-02, -1.17807806e-01, -3.62407506e-01,\n",
       "        -8.78731254e-03,  3.78472134e-02,  3.03632885e-01,  3.31887484e-01,\n",
       "        -2.37061866e-02,  4.46945727e-01,  6.77707493e-02, -5.49764037e-02,\n",
       "         1.96860544e-02, -2.08384022e-02, -3.51476640e-01, -3.89335416e-02,\n",
       "        -1.26035675e-01, -1.92590490e-01, -7.43549615e-02,  6.03984952e-01,\n",
       "         1.65721066e-02, -2.09062293e-01, -2.02109978e-01,  2.74238497e-01,\n",
       "        -4.21539620e-02, -9.34476182e-02, -1.12527788e-01, -4.37690802e-02,\n",
       "         3.20241481e-01,  1.87417924e-01,  1.65851906e-01, -3.21288824e-01,\n",
       "         1.32573664e-01,  1.43941551e-01, -2.81410366e-01,  4.38390195e-01,\n",
       "        -8.56281072e-02, -5.95043674e-02, -3.10507476e-01, -1.27893433e-01,\n",
       "         3.63643914e-01, -2.94273078e-01,  1.85540810e-01,  1.46885544e-01,\n",
       "         3.02295059e-01, -2.87707653e-02, -1.78747743e-01,  1.49600357e-01,\n",
       "        -4.70134318e-02, -2.26842016e-01,  1.36290148e-01,  1.00924574e-01,\n",
       "         1.71819776e-02,  1.51440933e-01, -2.67116070e-01, -3.37142229e-01,\n",
       "        -2.88031787e-01, -1.49303779e-01,  1.13835230e-01, -3.88404876e-01,\n",
       "         6.82893321e-02, -8.62707049e-02, -2.81706452e-01,  1.99428961e-01,\n",
       "        -5.90418354e-02, -3.26257721e-02,  1.51166409e-01,  9.50775594e-02,\n",
       "        -4.22544062e-01, -1.78817227e-01,  1.08430043e-01,  2.31927745e-02,\n",
       "        -1.30293623e-01, -1.34806126e-01,  1.23686150e-01, -6.52789891e-01,\n",
       "         1.40653342e-01,  1.18304595e-01, -1.85870737e-01,  1.69027373e-01,\n",
       "         6.22398444e-02, -3.01028877e-01,  5.52272312e-02, -1.59732074e-01,\n",
       "        -6.07306473e-02,  2.59393603e-02,  5.93991093e-02, -4.09109652e-01,\n",
       "         5.65277711e-02,  8.26695934e-02, -1.09665036e-01,  5.24623513e-01,\n",
       "        -2.99178630e-01,  4.02029127e-01,  7.95439556e-02,  3.09466422e-02,\n",
       "        -1.64480388e-01, -3.01514685e-01,  2.43446082e-01, -4.41939026e-01,\n",
       "        -4.04028565e-01,  3.07424907e-02, -2.35029295e-01,  5.96452355e-02,\n",
       "         2.47819927e-02, -1.06562279e-01, -4.57943752e-02,  1.77037790e-01,\n",
       "         1.12932160e-01,  3.97829302e-02,  1.21606529e-01,  2.07717374e-01,\n",
       "         4.92224135e-02, -3.78449976e-01,  2.32376438e-02, -1.00620314e-01,\n",
       "         1.03461727e-01, -1.55636102e-01,  7.76971271e-03,  1.01058103e-01,\n",
       "        -2.35684127e-01, -4.14852709e-01,  4.68117893e-01, -2.71661103e-01,\n",
       "        -8.94096643e-02,  7.73058459e-02,  3.23934853e-02, -1.37638405e-01,\n",
       "         2.35883877e-01, -2.24919707e-01, -6.60033822e-02,  2.98644841e-01,\n",
       "        -1.10857889e-01, -9.78285745e-02,  2.06312612e-01,  1.63963392e-01,\n",
       "        -1.26688033e-01, -1.44172115e-02, -2.41080876e-02,  3.87236662e-02,\n",
       "         4.96663988e-01,  2.21446641e-02,  1.82330430e-01,  1.16293475e-01,\n",
       "        -5.14086299e-02, -1.09183528e-01,  1.99053541e-01,  1.13521792e-01,\n",
       "        -1.42657652e-01,  1.17103294e-01,  2.92870756e-02, -3.63938510e-01,\n",
       "         2.10203812e-01,  2.53999472e-01, -1.91400260e-01, -3.47789466e-01,\n",
       "         6.54853046e-01,  3.44411224e-01, -2.42294118e-01, -3.76438707e-01,\n",
       "         7.29219168e-02, -2.58186400e-01, -1.16382696e-01, -7.02646142e-03,\n",
       "         1.02566279e-01, -2.17843391e-02,  6.29281402e-01,  3.20924562e-03,\n",
       "         1.18414178e-01,  2.17285827e-01, -3.85146350e-01, -1.30077749e-01,\n",
       "        -1.58804916e-02,  2.05102876e-01, -4.92855087e-02,  2.70921648e-01,\n",
       "        -3.37666124e-01,  4.61389244e-01, -1.90050274e-01, -1.54395074e-01,\n",
       "        -3.61221805e-02, -5.64175770e-02,  1.76261380e-01,  1.41626298e-02,\n",
       "        -1.25995532e-01,  8.36598873e-02,  3.57335627e-01,  4.73096997e-01,\n",
       "         2.78240353e-01,  1.96310356e-01,  1.71880841e-01, -4.67333421e-02,\n",
       "         7.25453734e-01,  4.84922171e-01,  1.57174245e-01,  4.07931626e-01,\n",
       "        -1.09809995e-01, -2.14719132e-01,  9.79388207e-02,  3.02587032e-01,\n",
       "         2.93800175e-01,  2.10601375e-01, -8.68891180e-02,  4.97211754e-01,\n",
       "         7.28913099e-02,  1.24988213e-01,  1.41549215e-01, -3.42009813e-01,\n",
       "        -1.94295317e-01,  1.94208398e-01,  2.15863183e-01, -2.29483411e-01,\n",
       "        -2.14761406e-01, -5.06942272e-02, -7.02634314e-03,  6.85542682e-03,\n",
       "        -2.39959568e-01, -1.13460973e-01, -3.48520041e-01,  3.67345750e-01,\n",
       "         7.13437274e-02, -1.00696310e-01, -1.63186789e-01,  2.16520391e-02,\n",
       "        -6.61933571e-02, -1.07898369e-01, -3.77926171e-01, -1.02714561e-01,\n",
       "        -1.30681902e-01, -6.35236084e-01, -5.14115691e-02,  7.34579265e-02,\n",
       "        -1.46639183e-01, -3.43350679e-01, -8.20860118e-02, -5.99759333e-02,\n",
       "         1.19505048e-01, -1.69444337e-01,  3.51632237e-02, -2.74460822e-01,\n",
       "         4.03105170e-01,  2.28763819e-01,  3.72814506e-01,  1.10749185e-01,\n",
       "         1.00562692e-01, -1.93510473e-01, -8.31989422e-02,  1.54113039e-01,\n",
       "         1.85273021e-01,  1.44894078e-01, -1.17516648e-02,  3.04703295e-01,\n",
       "        -3.23484868e-01,  3.06809898e-02, -1.61133278e-02,  4.87714052e-01,\n",
       "        -7.10440695e-01,  1.67573422e-01, -7.61084259e-02, -3.13271806e-02,\n",
       "         1.10023461e-01,  7.49231949e-02, -2.31193468e-01,  2.31067628e-01,\n",
       "        -3.20811689e-01, -2.28275985e-01,  3.19196358e-02,  2.01747447e-01,\n",
       "        -2.30058461e-01, -2.73524020e-02,  5.84050491e-02,  3.10509890e-01,\n",
       "        -1.36138171e-01, -8.22571069e-02,  7.94780068e-03,  2.40590885e-01,\n",
       "         1.24435402e-01,  1.03558943e-01, -1.13159440e-01, -3.14568609e-01,\n",
       "        -1.40879214e-01, -1.88385136e-02, -2.14695618e-01,  4.27041836e-02,\n",
       "         1.90246135e-01,  1.69376165e-01,  1.17511917e-02, -7.89344981e-02,\n",
       "         3.90662670e-01, -3.58959660e-02,  3.71633470e-02, -1.71231642e-01,\n",
       "        -1.29156813e-01, -3.89921039e-01, -8.16350654e-02,  2.99374070e-02,\n",
       "        -2.24052556e-02, -1.89069957e-01, -8.82945508e-02, -1.06781222e-01,\n",
       "        -1.69745371e-01, -1.43819958e-01,  1.38884619e-01, -3.42924520e-03],\n",
       "       dtype=float32),\n",
       " array([ 7.59631991e-02,  1.91988766e-01,  2.56202251e-01, -1.41672224e-01,\n",
       "         9.21031088e-03, -2.50589937e-01,  1.19698837e-01,  6.23776793e-01,\n",
       "        -7.76555203e-03, -2.28730753e-01,  1.75309569e-01, -4.19514418e-01,\n",
       "        -2.18413055e-01,  3.90718043e-01, -2.97421813e-01,  5.07364988e-01,\n",
       "        -1.39833847e-02, -3.91575173e-02, -1.76586315e-01,  3.08081746e-01,\n",
       "         3.35715741e-01,  1.26576656e-02, -6.79496825e-02,  3.56896281e-01,\n",
       "         3.43357056e-01,  1.10919587e-02,  1.20636523e-02, -5.45169115e-02,\n",
       "        -2.58787274e-01, -1.15567535e-01,  4.95005310e-01,  7.12050796e-02,\n",
       "        -6.15967512e-02, -6.22691847e-02,  1.40095165e-03, -4.05795462e-02,\n",
       "        -7.60384649e-02,  3.41450870e-02, -1.49795830e-01,  1.87782347e-01,\n",
       "        -5.00286102e-01, -2.07735956e-01,  1.98167060e-02,  1.00003682e-01,\n",
       "        -2.58066058e-01, -3.74936402e-01,  2.86641777e-01, -1.28018677e-01,\n",
       "         1.10793605e-01, -1.28706440e-01, -3.07307750e-01,  1.69347465e-01,\n",
       "        -3.18366736e-01, -1.17217921e-01,  1.84790626e-01,  6.21015489e-01,\n",
       "        -7.29372352e-02, -6.07106209e-01, -5.74439108e-01, -7.10827634e-02,\n",
       "         2.24888071e-01, -1.14420978e-02,  1.03633381e-01, -2.73457855e-01,\n",
       "         1.64069071e-01,  2.42199466e-01, -3.50575596e-02,  4.13653463e-01,\n",
       "        -7.02135980e-01,  9.10883490e-03, -3.28332007e-01, -5.80444448e-02,\n",
       "        -1.10147789e-01,  2.47563571e-02, -1.09091975e-01, -7.71198496e-02,\n",
       "        -7.80177712e-02,  3.37756366e-01, -3.60495001e-02, -8.44275504e-02,\n",
       "        -2.23642930e-01,  2.86354780e-01, -3.86076242e-01,  3.89594704e-01,\n",
       "         3.83355096e-02,  8.81237909e-02,  1.27971461e-02,  1.00066811e-01,\n",
       "        -3.33972752e-01,  4.14549440e-01,  2.46391043e-01, -2.70061702e-01,\n",
       "         5.73251173e-02, -3.78894247e-03,  2.14510590e-01, -2.08237052e-01,\n",
       "         2.54928052e-01,  7.38612637e-02, -1.54640809e-01,  3.70723099e-01,\n",
       "         2.01048076e-01, -1.70422316e-01,  9.17713121e-02, -1.44004866e-01,\n",
       "         1.05331130e-02, -1.56285316e-01,  1.33603960e-01,  2.38074347e-01,\n",
       "         1.93593889e-01,  1.02911867e-01,  1.28597170e-01, -8.87671039e-02,\n",
       "        -1.47581503e-01, -3.45491946e-01, -1.80829883e-01,  3.26596200e-02,\n",
       "         1.29913166e-01, -1.47749126e-01,  1.04832135e-01,  8.97029415e-02,\n",
       "        -5.77161871e-02, -9.61003676e-02,  1.04275823e-01,  7.40448475e-01,\n",
       "         7.63313696e-02,  3.16348076e-02, -1.87352106e-01,  2.81357139e-01,\n",
       "        -1.57055110e-01, -1.48492455e-01,  1.85152844e-01,  2.58773237e-01,\n",
       "         1.13334946e-01, -3.96985769e-01, -3.23063999e-01,  1.61863670e-01,\n",
       "         9.39674973e-02, -3.81294012e-01, -2.84937888e-01,  3.70492488e-02,\n",
       "         4.03504707e-02, -1.98009759e-01,  1.72379166e-01,  1.09785885e-01,\n",
       "        -1.46884667e-02,  5.75905852e-02, -1.29224107e-01, -7.40854293e-02,\n",
       "         9.05333757e-02,  8.70662853e-02,  5.15096970e-02, -1.38576746e-01,\n",
       "        -5.93630373e-02, -1.88204810e-01, -1.77792698e-01, -5.80293387e-02,\n",
       "        -2.35896140e-01,  1.50618121e-01, -4.40464132e-02,  1.68005243e-01,\n",
       "         4.65065390e-01, -1.98632002e-01, -1.11061074e-01,  2.86953449e-01,\n",
       "         2.64811784e-01, -5.00466935e-02,  1.48728094e-03,  1.86437845e-01,\n",
       "        -6.85943589e-02,  2.79728584e-02, -3.43101472e-02, -1.24723785e-01,\n",
       "         8.56642365e-01, -2.10842229e-02,  1.05412431e-01, -8.57405886e-02,\n",
       "         2.47041002e-01,  1.57165021e-01,  1.47199795e-01,  8.00386444e-02,\n",
       "        -6.24597371e-01,  3.30061048e-01, -5.59264123e-02,  1.55737743e-01,\n",
       "         1.98694870e-01, -9.49902534e-02,  1.37869895e-01, -2.50169277e-01,\n",
       "         1.83026537e-01,  2.69162413e-02, -3.56970280e-01, -2.27863416e-01,\n",
       "        -2.75431842e-01, -1.68325175e-02,  3.04053873e-01, -4.08316344e-01,\n",
       "        -2.89080262e-01, -9.20233727e-02, -4.10124063e-01,  2.10209176e-01,\n",
       "         8.25103223e-02,  6.23111650e-02,  1.81806251e-01,  2.04529881e-01,\n",
       "        -2.79552460e-01, -2.85918708e-03,  1.72032222e-01, -5.37289195e-02,\n",
       "        -9.19218063e-02,  4.11131859e-01, -1.34740844e-01,  3.25432777e-01,\n",
       "        -7.16974586e-02,  1.78390648e-02, -1.21807612e-01,  7.51155317e-02,\n",
       "         5.12192678e-03, -1.04601264e-01,  1.00531064e-01, -2.40623159e-03,\n",
       "         7.95914158e-02, -2.57013701e-02, -4.85945493e-01,  3.21200460e-01,\n",
       "         2.85401214e-02,  5.14856577e-01, -7.40063488e-02, -4.35312986e-01,\n",
       "         1.77568868e-01,  2.83975869e-01, -6.93102507e-03, -2.59623170e-01,\n",
       "         6.32907927e-01, -3.36817466e-03, -4.72777896e-02,  1.11739345e-01,\n",
       "        -1.26779065e-01, -2.40743056e-01,  1.54242292e-01, -1.40976548e-01,\n",
       "        -2.65283823e-01,  3.87162924e-01,  2.51130283e-01,  2.60216147e-01,\n",
       "         2.49842051e-02,  4.18068608e-03, -2.94651419e-01, -9.12750661e-02,\n",
       "        -1.19150341e-01, -5.64535484e-02, -3.88272643e-01, -2.98534036e-01,\n",
       "         4.08454705e-03, -5.03842592e-01, -3.57215293e-02, -2.61586338e-01,\n",
       "        -3.11862230e-01, -1.80744186e-01, -7.55700245e-02, -2.40214472e-03,\n",
       "         2.46971533e-01,  1.42300189e-01, -6.26609698e-02,  1.73212394e-01,\n",
       "        -3.36365759e-01, -3.26783836e-01,  1.80373657e-02,  3.58117908e-01,\n",
       "         1.29096866e-01,  1.76043779e-01,  9.50676668e-03, -7.46679679e-02,\n",
       "        -4.96270396e-02,  4.46311206e-01, -2.43576884e-01, -2.08259765e-02,\n",
       "         2.30263010e-01,  1.23330779e-01, -1.47408452e-02, -1.94669932e-01,\n",
       "         1.82020709e-01,  4.19570625e-01, -3.45101207e-01,  9.19476822e-02,\n",
       "         2.44610235e-02, -3.41447771e-01,  1.32482007e-01,  3.79112631e-01,\n",
       "        -3.43653172e-01, -2.75321931e-01, -8.91507566e-02,  1.71662927e-01,\n",
       "        -3.33338767e-01, -1.94001958e-01,  3.49044800e-01, -9.76466015e-03,\n",
       "         1.47086367e-01,  1.03443123e-01,  2.22191989e-01, -2.70658791e-01,\n",
       "        -2.28452355e-01,  2.21392512e-02,  2.13143155e-01,  3.08285832e-01,\n",
       "        -1.96713150e-01,  1.52364850e-01, -1.59354433e-01, -3.28269094e-01,\n",
       "        -3.63370228e+00, -1.07087819e-02,  2.15401292e-01, -3.12619686e-01,\n",
       "         3.45788002e-01, -8.04238617e-02,  7.03850612e-02, -2.17346385e-01,\n",
       "        -4.72825408e-01, -8.35577026e-02,  6.54595089e-04, -2.30082840e-01,\n",
       "         4.08026934e-01,  1.93789914e-01,  2.04074234e-01,  1.04704209e-01,\n",
       "         1.25211075e-01, -2.95735747e-01, -3.07051569e-01,  5.92167318e-01,\n",
       "        -8.60310644e-02, -1.81014046e-01,  2.07595736e-01, -1.18189648e-01,\n",
       "         3.58917743e-01,  4.89662766e-01, -1.67295173e-01, -1.02320081e-02,\n",
       "        -1.64535478e-01, -2.17908025e-01,  2.27030478e-02, -2.47806996e-01,\n",
       "        -3.61350477e-02,  1.41511813e-01,  1.19734071e-01,  2.24380987e-03,\n",
       "         4.21253091e-04, -3.00612211e-01, -1.18909068e-01, -2.43141606e-01,\n",
       "        -4.98210490e-02, -6.45303071e-01, -4.71607223e-02, -1.16015963e-01,\n",
       "         7.19923079e-01, -2.20341280e-01, -1.67688087e-01, -3.30372930e-01,\n",
       "        -6.93261484e-03,  1.50934294e-01, -1.49258692e-03, -1.32721439e-01,\n",
       "        -2.20689878e-01, -1.05746783e-01, -5.82912974e-02,  1.58990398e-01,\n",
       "         3.75894904e-01,  6.01251662e-01, -1.89143896e-01, -5.06651402e-01,\n",
       "        -3.36266309e-02, -1.62368298e-01, -5.40828228e-01, -3.34478095e-02,\n",
       "        -9.13903117e-02, -2.04699859e-01, -4.14506853e-01, -1.85285345e-01,\n",
       "         1.74714282e-01,  4.03727479e-02,  1.79135632e-02,  3.31681132e-01,\n",
       "        -2.97975987e-01, -6.40849590e-01, -1.37978181e-01,  1.08506961e-03,\n",
       "        -1.31770283e-01, -7.92606175e-02,  1.31884828e-01, -6.64067715e-02,\n",
       "        -2.31796414e-01, -1.98029518e-01, -5.75744472e-02, -1.62200958e-01,\n",
       "        -2.02316523e-01, -3.43443274e-01,  9.44885164e-02, -1.37118250e-01,\n",
       "        -2.88686037e-01, -4.89250898e-01,  2.19537616e-01,  2.22082317e-01,\n",
       "         7.47768581e-02,  2.61451483e-01,  2.13718221e-01,  7.04363510e-02,\n",
       "         3.22520107e-01, -2.05736801e-01,  1.73182607e-01, -1.94162562e-01,\n",
       "         1.45173475e-01,  7.19544441e-02,  6.67681813e-01, -2.47487947e-01,\n",
       "         1.49247214e-01,  1.18918397e-01, -3.47315192e-01,  8.82795602e-02,\n",
       "         9.64012579e-04,  4.79491055e-02,  3.50388527e-01, -5.21293163e-01,\n",
       "         4.73465115e-01, -2.40602925e-01, -3.68427306e-01, -2.17509598e-01,\n",
       "         2.92617202e-01,  3.50557983e-01, -1.01475902e-01, -2.56050736e-01,\n",
       "        -2.23946702e-02,  4.58677202e-01, -5.25217175e-01, -5.09087622e-01,\n",
       "        -6.83022022e-01, -4.83760275e-02, -2.00854003e-01,  9.98737812e-02,\n",
       "        -1.13852881e-01,  1.16041347e-01, -1.09680086e-01, -1.93899587e-01,\n",
       "        -2.26008773e-01,  3.06201190e-01,  9.20270905e-02, -8.29765350e-02,\n",
       "        -9.33522806e-02, -4.52772498e-01, -9.91183221e-02,  7.02530295e-02,\n",
       "         7.37440512e-02,  1.74467117e-01, -9.57876667e-02,  4.88767549e-02,\n",
       "        -1.41143173e-01,  3.06479394e-01, -1.11373626e-01, -1.76242590e-01,\n",
       "        -7.81777650e-02,  1.59816936e-01, -2.57674545e-01, -3.46812934e-01,\n",
       "         1.09198466e-02, -3.47292542e-01,  2.53231496e-01,  2.58091182e-01,\n",
       "         2.43106410e-01, -9.00563151e-02, -7.36081600e-02, -3.88309598e-01,\n",
       "         6.68441579e-02,  1.13910362e-01,  1.40905857e-01,  2.69318849e-01,\n",
       "         3.77384536e-02,  4.07756269e-01,  8.22642120e-04, -9.36623365e-02,\n",
       "        -4.09047678e-02,  1.61617380e-02, -3.27210844e-01, -1.81985945e-02,\n",
       "         1.81120578e-02, -1.41505316e-01, -6.06714226e-02,  5.65850079e-01,\n",
       "         2.67842319e-02, -2.03907058e-01, -1.05538759e-02,  1.98331520e-01,\n",
       "        -1.01152107e-01, -1.66487500e-01, -7.93717504e-02, -2.21258011e-02,\n",
       "         3.08601499e-01,  1.55781573e-02,  1.90350190e-01, -1.70612857e-01,\n",
       "         1.51486024e-01,  1.00159802e-01, -4.79977012e-01,  4.63897198e-01,\n",
       "        -2.98171509e-02,  6.89933598e-02, -3.51235956e-01, -8.43623430e-02,\n",
       "         3.81814837e-01, -4.05800879e-01,  1.25132084e-01,  2.34220728e-01,\n",
       "         4.15721029e-01, -1.97662003e-02, -1.49614349e-01,  1.60513461e-01,\n",
       "        -8.98276493e-02, -2.01883018e-01,  5.96777461e-02,  8.03047139e-03,\n",
       "        -2.42928931e-04,  2.12755010e-01, -3.04427385e-01, -3.64732116e-01,\n",
       "        -3.17499459e-01, -5.23833670e-02,  1.41670927e-01, -4.37391073e-01,\n",
       "         6.48330897e-02, -1.58850297e-01, -4.00702983e-01,  2.96563596e-01,\n",
       "        -4.75876145e-02, -1.19281843e-01,  6.78417608e-02,  1.40176728e-01,\n",
       "        -4.87459183e-01, -1.16389036e-01, -9.46760178e-03,  5.59187829e-02,\n",
       "        -9.32528675e-02, -1.03170134e-01, -4.65674028e-02, -6.48798466e-01,\n",
       "         1.87444448e-01,  2.27956548e-01, -1.66881904e-01,  1.36194900e-01,\n",
       "         1.07713260e-01, -2.07266554e-01,  5.44855706e-02, -5.88762201e-02,\n",
       "        -7.44017139e-02,  1.61408573e-01, -3.95157374e-02, -4.56281006e-01,\n",
       "         1.83557242e-01,  6.57775626e-02, -6.69231489e-02,  4.68668371e-01,\n",
       "        -3.17725480e-01,  3.46894711e-01,  1.16675325e-01,  4.57228273e-02,\n",
       "        -9.41528678e-02, -3.60324204e-01,  1.99327216e-01, -5.32008171e-01,\n",
       "        -4.55891758e-01,  7.84057975e-02, -1.52262077e-01,  7.54880980e-02,\n",
       "        -1.06252871e-01, -9.74508449e-02, -5.68126701e-02,  1.48153991e-01,\n",
       "        -2.03687022e-03,  6.14330769e-02,  2.05935016e-01,  2.24021554e-01,\n",
       "        -8.87897313e-02, -3.38395566e-01, -9.75534040e-03, -1.54366523e-01,\n",
       "         1.43581733e-01, -2.13874042e-01, -5.95367700e-02,  1.20423481e-01,\n",
       "        -2.52908319e-01, -3.98058861e-01,  5.77366471e-01, -2.90229678e-01,\n",
       "        -6.83517158e-02,  6.30388409e-02,  6.14033453e-02, -2.49027252e-01,\n",
       "         1.97253361e-01, -2.55999625e-01, -1.13968626e-02,  2.34050855e-01,\n",
       "        -2.40782514e-01, -1.05107211e-01,  2.53415227e-01,  2.24227726e-01,\n",
       "        -2.61245258e-02, -3.46483476e-02, -2.81671784e-03,  1.45051837e-01,\n",
       "         4.71405476e-01,  8.68509896e-03,  1.49808928e-01,  8.75044838e-02,\n",
       "        -4.37209476e-03, -1.21838473e-01,  2.31600806e-01,  1.68661669e-01,\n",
       "        -1.44075319e-01,  8.31401497e-02,  7.63803571e-02, -3.96781415e-01,\n",
       "         2.33568713e-01,  2.46562704e-01, -2.39833236e-01, -4.11335677e-01,\n",
       "         6.52501106e-01,  2.32465088e-01, -2.29206085e-01, -3.29795986e-01,\n",
       "         6.30109310e-02, -2.21346378e-01, -1.13035738e-01, -1.94051191e-02,\n",
       "         9.93936509e-02, -5.10579832e-02,  6.40664577e-01,  2.82243434e-02,\n",
       "         2.88070917e-01,  2.01369569e-01, -3.71919543e-01, -1.15237087e-01,\n",
       "         1.24013953e-01,  2.74155527e-01, -8.44125748e-02,  2.24696010e-01,\n",
       "        -2.80245215e-01,  4.73138422e-01, -1.13056473e-01, -7.71464854e-02,\n",
       "        -6.26951456e-03, -1.00195266e-01,  1.46898314e-01,  7.63868913e-02,\n",
       "        -1.30097389e-01,  1.19131304e-01,  4.00435120e-01,  5.83743274e-01,\n",
       "         2.69425094e-01,  2.39075705e-01,  1.24449655e-01, -3.03422082e-02,\n",
       "         7.48526573e-01,  4.65983182e-01,  1.61610618e-01,  4.39855009e-01,\n",
       "        -9.70103815e-02, -2.18150392e-01,  9.53198746e-02,  3.00778180e-01,\n",
       "         2.35984877e-01,  2.94644415e-01, -2.08124235e-01,  4.83285099e-01,\n",
       "         1.39621213e-01,  4.64772508e-02,  1.56756207e-01, -3.72163594e-01,\n",
       "        -1.91415861e-01,  1.58857390e-01,  1.33943930e-01, -2.57628143e-01,\n",
       "        -1.91926792e-01, -4.02216241e-02, -8.52021724e-02, -2.99644433e-02,\n",
       "        -3.25084388e-01, -5.49832098e-02, -3.77852112e-01,  3.61570418e-01,\n",
       "         8.91363621e-02, -1.27626717e-01, -1.00789711e-01,  4.49960940e-02,\n",
       "        -3.85615677e-02, -4.60199416e-02, -4.44431394e-01, -1.46422997e-01,\n",
       "        -4.84732315e-02, -5.28331041e-01, -1.05226547e-01,  4.22793441e-02,\n",
       "        -1.05849206e-01, -3.40750813e-01, -1.33526683e-01, -7.81848282e-02,\n",
       "         4.17715535e-02, -2.43028998e-01,  1.19767077e-01, -2.47703254e-01,\n",
       "         4.35430229e-01,  2.09498838e-01,  3.92856687e-01,  1.24311887e-01,\n",
       "         7.53028840e-02, -1.79954335e-01, -4.98575792e-02,  1.95571750e-01,\n",
       "         1.87629491e-01,  2.12610140e-01, -6.22261204e-02,  2.21135810e-01,\n",
       "        -3.03079337e-01, -5.95762730e-02, -6.95864707e-02,  4.76226896e-01,\n",
       "        -7.03398466e-01,  1.75743148e-01, -7.56182745e-02, -2.01282725e-02,\n",
       "         6.16549961e-02,  2.50055324e-02, -1.91247076e-01,  2.10296214e-01,\n",
       "        -3.63972783e-01, -2.68145621e-01,  1.29301967e-02,  1.55824229e-01,\n",
       "        -2.24973530e-01,  2.55227834e-02,  2.07990959e-01,  3.29335123e-01,\n",
       "        -8.32308084e-02, -5.82382977e-02,  5.68219498e-02,  2.17678577e-01,\n",
       "         7.74252266e-02,  2.14408003e-02, -7.46562704e-02, -3.02014679e-01,\n",
       "        -2.06877276e-01, -9.61074140e-03, -2.74972349e-01, -6.11439813e-03,\n",
       "         2.37722501e-01,  6.21561520e-02, -4.90263775e-02, -1.41472071e-01,\n",
       "         3.34989220e-01, -7.23574534e-02, -1.32069702e-03, -2.25319982e-01,\n",
       "        -1.45380184e-01, -2.62645811e-01, -8.66649374e-02, -2.15122551e-02,\n",
       "         4.14634049e-02, -2.82673955e-01, -2.64552049e-02, -1.17050387e-01,\n",
       "        -2.66912580e-02, -1.80189803e-01,  1.41730264e-01, -1.12341549e-02],\n",
       "       dtype=float32),\n",
       " array([ 4.21804488e-02,  9.42772329e-02,  2.12955683e-01, -1.01822093e-01,\n",
       "         4.84873280e-02, -2.75029629e-01,  8.53066519e-02,  6.80252135e-01,\n",
       "        -3.93940136e-02, -1.62527874e-01,  1.22317351e-01, -3.87603939e-01,\n",
       "        -2.28797838e-01,  4.29366887e-01, -2.80495673e-01,  4.19921786e-01,\n",
       "        -4.98973653e-02,  2.34667268e-02, -1.27280205e-01,  2.57438481e-01,\n",
       "         2.06643626e-01,  3.59577462e-02, -3.21037211e-02,  2.94834197e-01,\n",
       "         3.42294514e-01, -6.08098507e-02,  2.19615288e-02, -4.47493717e-02,\n",
       "        -2.48414427e-01, -1.38999879e-01,  4.50392812e-01,  1.57649383e-01,\n",
       "        -1.74710616e-01, -8.09699148e-02, -7.04114810e-02, -4.25138064e-02,\n",
       "        -3.29629369e-02, -2.57356577e-02, -1.01562999e-01,  2.21032336e-01,\n",
       "        -5.10244131e-01, -2.10012868e-01,  4.83610341e-03,  6.99303672e-02,\n",
       "        -2.12779254e-01, -3.92091334e-01,  2.32442707e-01, -9.37702209e-02,\n",
       "         8.33291039e-02, -1.56468689e-01, -3.27485889e-01,  1.00121573e-01,\n",
       "        -2.03466028e-01, -7.57958740e-02,  1.35413617e-01,  6.35951281e-01,\n",
       "        -1.55192778e-01, -5.89805722e-01, -5.22796452e-01, -8.78118128e-02,\n",
       "         3.10460150e-01, -8.40081722e-02,  2.54155807e-02, -3.57249677e-01,\n",
       "         2.31336206e-01,  1.83122009e-01, -1.63182560e-02,  4.02428716e-01,\n",
       "        -6.79462790e-01, -3.98153737e-02, -3.78771544e-01, -1.88169762e-01,\n",
       "        -1.79645687e-01,  1.43106012e-02, -3.42493318e-02,  6.46411031e-02,\n",
       "        -1.55523434e-01,  3.40401977e-01, -3.47597967e-03, -1.79730296e-01,\n",
       "        -2.48113275e-01,  3.46525759e-01, -3.25957954e-01,  3.64122957e-01,\n",
       "         5.31734005e-02,  2.77705733e-02,  1.12205725e-02,  9.86483991e-02,\n",
       "        -3.39617372e-01,  3.77887934e-01,  2.00611085e-01, -3.20450723e-01,\n",
       "         8.40770155e-02, -5.09298369e-02,  1.77195385e-01, -1.18013635e-01,\n",
       "         2.35104173e-01,  1.23057149e-01, -1.33890778e-01,  5.35043597e-01,\n",
       "         1.59476668e-01, -1.42711625e-01,  8.28647986e-02, -8.58355761e-02,\n",
       "        -2.89747305e-02, -2.01875120e-01,  2.13309199e-01,  2.85188109e-01,\n",
       "         1.64692417e-01,  1.55561611e-01,  1.04978658e-01, -1.17589615e-01,\n",
       "        -5.38776740e-02, -3.08139503e-01, -2.25493938e-01,  1.25157461e-01,\n",
       "         1.58533588e-01, -1.76084846e-01,  8.31936449e-02,  1.57847852e-01,\n",
       "        -3.86542305e-02, -9.93789360e-02,  6.67176321e-02,  7.91341543e-01,\n",
       "         1.60358171e-03,  1.78393155e-01, -1.68847799e-01,  2.48393387e-01,\n",
       "        -1.47486150e-01, -1.38809413e-01,  1.00162864e-01,  3.81528914e-01,\n",
       "         1.65819675e-01, -4.64381307e-01, -2.07711026e-01,  1.61304444e-01,\n",
       "         5.97562864e-02, -3.48195493e-01, -3.99934202e-01,  3.10318116e-02,\n",
       "         4.96121459e-02, -6.73375875e-02,  1.61037132e-01,  9.89221781e-02,\n",
       "         7.99336731e-02,  1.51058555e-01, -1.80448681e-01, -1.08059905e-01,\n",
       "         5.75092435e-02,  9.18478966e-02, -8.36298428e-03, -1.07147790e-01,\n",
       "        -6.63443208e-02, -1.40218288e-01, -1.68732136e-01, -9.78466123e-02,\n",
       "        -2.71190166e-01,  1.52992204e-01, -1.24503769e-01,  1.70364931e-01,\n",
       "         4.99017149e-01, -1.28664643e-01, -1.07925750e-01,  2.54460663e-01,\n",
       "         2.31404930e-01,  9.57042724e-03, -1.29951369e-02,  2.65836358e-01,\n",
       "        -8.94084424e-02,  4.88163754e-02, -1.47729842e-02, -1.89548284e-01,\n",
       "         8.96945179e-01, -9.00819972e-02,  1.20476387e-01, -7.32333958e-02,\n",
       "         2.27353975e-01,  1.71901494e-01,  1.28625184e-01,  1.09597765e-01,\n",
       "        -6.42731786e-01,  3.42254370e-01, -1.95372291e-02,  1.04160950e-01,\n",
       "         2.52869874e-01, -4.06346954e-02,  4.16203514e-02, -2.02462390e-01,\n",
       "         1.45918161e-01, -3.11455168e-02, -2.46974990e-01, -1.75932884e-01,\n",
       "        -3.47691894e-01,  2.17614323e-03,  3.01720679e-01, -3.93173516e-01,\n",
       "        -2.17562765e-01,  2.94833770e-03, -3.69561642e-01,  1.72190383e-01,\n",
       "         1.31237701e-01,  5.08269966e-02,  2.43081212e-01,  2.12569639e-01,\n",
       "        -2.27440745e-01,  3.15940715e-02,  1.16796806e-01, -9.05812830e-02,\n",
       "        -8.68050158e-02,  4.35408056e-01, -1.46379188e-01,  4.51814175e-01,\n",
       "         1.32644204e-02, -3.22631374e-02, -9.80344936e-02, -1.34131759e-02,\n",
       "         7.56970868e-02, -1.86059177e-01,  5.08267768e-02,  4.06488404e-03,\n",
       "         7.98733085e-02, -9.29868147e-02, -4.08855379e-01,  3.59013319e-01,\n",
       "        -5.36959395e-02,  5.95372200e-01, -2.71221244e-04, -4.18507248e-01,\n",
       "         1.92044616e-01,  3.63376319e-01, -1.50724072e-02, -2.00871393e-01,\n",
       "         5.99906802e-01, -9.99068934e-03, -1.04210064e-01,  5.94834611e-02,\n",
       "        -1.05170682e-01, -2.48174816e-01,  9.71941501e-02, -2.60478348e-01,\n",
       "        -2.40613505e-01,  3.76714408e-01,  2.20134944e-01,  2.15503618e-01,\n",
       "         5.07920310e-02, -5.47559699e-03, -2.30066493e-01, -1.37708545e-01,\n",
       "        -2.07207888e-01, -1.30243540e-01, -3.74222577e-01, -4.16056067e-01,\n",
       "        -5.18983826e-02, -5.66919565e-01, -1.02699302e-01, -1.89740971e-01,\n",
       "        -2.39052743e-01, -1.72506645e-01, -8.09996948e-02,  3.54454741e-02,\n",
       "         2.69400656e-01,  8.88584405e-02, -9.88785252e-02,  1.53473705e-01,\n",
       "        -5.21082044e-01, -3.94808441e-01,  4.94714901e-02,  3.08117568e-01,\n",
       "         1.38239756e-01,  6.67438507e-02, -4.97016720e-02, -1.11077353e-01,\n",
       "        -1.43305929e-02,  5.18557906e-01, -2.63849974e-01, -3.94227877e-02,\n",
       "         2.46725708e-01,  2.45555446e-01,  2.73055080e-02, -1.90041363e-01,\n",
       "         1.84600025e-01,  4.83010292e-01, -2.21212864e-01,  1.14606656e-01,\n",
       "        -3.83946076e-02, -3.52334440e-01,  1.75161995e-02,  3.28677297e-01,\n",
       "        -2.91806370e-01, -2.74901181e-01, -1.14500955e-01,  1.02706075e-01,\n",
       "        -2.61247456e-01, -2.35908180e-01,  3.52557093e-01,  2.71297302e-02,\n",
       "         1.00600421e-01,  1.38817638e-01,  1.92412183e-01, -2.87915885e-01,\n",
       "        -2.70831466e-01,  1.14117779e-01,  1.72682375e-01,  2.47884065e-01,\n",
       "        -3.45654905e-01,  2.42569253e-01, -1.46825105e-01, -3.97262633e-01,\n",
       "        -3.83546829e+00,  2.39040907e-02,  1.79236457e-01, -2.88655907e-01,\n",
       "         2.58091718e-01, -1.94831371e-01,  6.92504942e-02, -1.94521517e-01,\n",
       "        -4.63058084e-01, -6.87531382e-02,  7.40937814e-02, -2.08166569e-01,\n",
       "         3.95956337e-01,  2.51696557e-01,  1.57192260e-01,  1.43716261e-01,\n",
       "         1.92640752e-01, -1.98577300e-01, -3.04304391e-01,  6.11149848e-01,\n",
       "        -9.65559036e-02, -2.48533443e-01,  2.41511390e-01, -2.52567858e-01,\n",
       "         4.27972257e-01,  4.19174582e-01, -1.64284617e-01,  4.00677770e-02,\n",
       "        -2.45943472e-01, -2.75762647e-01,  6.15702756e-02, -1.45404190e-01,\n",
       "         6.72959983e-02,  1.93057820e-01, -6.43581245e-03,  2.30388455e-02,\n",
       "         2.92897224e-02, -3.41519803e-01, -5.13283126e-02, -1.98897168e-01,\n",
       "        -9.16389227e-02, -6.10656559e-01,  1.73830055e-02, -1.53312981e-01,\n",
       "         6.71189964e-01, -2.52025694e-01, -1.20366290e-01, -2.11433500e-01,\n",
       "         5.91058247e-02,  2.01285288e-01,  4.05480042e-02, -1.34321168e-01,\n",
       "        -2.72785395e-01, -1.28118366e-01, -7.90613145e-02,  1.04060628e-01,\n",
       "         4.68150556e-01,  6.05149627e-01, -2.04673499e-01, -5.48972130e-01,\n",
       "        -2.27154642e-02, -6.12821952e-02, -5.84462762e-01, -6.23831674e-02,\n",
       "        -1.18154146e-01, -3.23014200e-01, -4.05352354e-01, -9.01462138e-02,\n",
       "         1.43441379e-01,  4.88571338e-02, -2.70861872e-02,  3.64851177e-01,\n",
       "        -2.61286229e-01, -7.28950560e-01, -1.20969549e-01, -1.33386077e-02,\n",
       "        -1.11277021e-01, -1.35038406e-01,  1.07748166e-01, -9.03481022e-02,\n",
       "        -2.56036162e-01, -1.43600196e-01, -8.57935399e-02, -7.69966990e-02,\n",
       "        -2.13539451e-01, -3.75914425e-01,  1.29994288e-01, -1.41808197e-01,\n",
       "        -2.95048982e-01, -4.52952385e-01,  2.73161888e-01,  1.77974463e-01,\n",
       "         5.35087101e-02,  2.82510728e-01,  1.60915464e-01,  6.23766184e-02,\n",
       "         4.01939452e-01, -1.95660636e-01,  2.75857210e-01, -2.05893323e-01,\n",
       "         1.79605052e-01,  2.56226119e-02,  7.03553677e-01, -2.43294790e-01,\n",
       "         2.30762050e-01,  8.64090174e-02, -3.59014630e-01,  1.64808646e-01,\n",
       "         3.22338156e-02,  2.99730245e-02,  3.23086947e-01, -5.91510296e-01,\n",
       "         4.27702993e-01, -2.31189281e-01, -3.67212713e-01, -1.93529561e-01,\n",
       "         3.43558580e-01,  3.83399099e-01, -9.25345644e-02, -1.21102437e-01,\n",
       "        -8.90500993e-02,  5.64604878e-01, -4.84839678e-01, -5.35627544e-01,\n",
       "        -6.74428642e-01,  7.15479371e-04, -3.05247515e-01, -1.44304689e-02,\n",
       "        -1.15363523e-01,  1.04341589e-01, -1.87999263e-01, -1.96305826e-01,\n",
       "        -1.56529903e-01,  2.70223260e-01,  9.82390568e-02, -9.03684795e-02,\n",
       "        -5.11971228e-02, -3.15753371e-01,  3.45377699e-02,  4.25636135e-02,\n",
       "         1.54720515e-01,  1.26142353e-01, -7.23931938e-02, -3.82212177e-02,\n",
       "        -7.24320263e-02,  2.94926643e-01, -8.32037777e-02, -2.01060727e-01,\n",
       "        -9.63910446e-02,  1.36193976e-01, -2.23006129e-01, -4.44744974e-01,\n",
       "        -2.47631921e-03, -3.25206429e-01,  2.21822217e-01,  7.94211999e-02,\n",
       "         2.24838927e-01, -3.55973132e-02, -1.12933218e-01, -4.47913021e-01,\n",
       "         2.12241244e-02,  1.01286806e-01,  2.78564364e-01,  2.60195494e-01,\n",
       "        -1.77597925e-02,  4.35741574e-01,  8.69293958e-02, -9.64227170e-02,\n",
       "        -1.04643106e-01,  7.56548271e-02, -3.32401693e-01,  3.39006186e-02,\n",
       "        -1.77115768e-01, -2.00349137e-01, -5.04604764e-02,  5.48362851e-01,\n",
       "        -1.42568825e-02, -1.40165612e-01, -9.87824872e-02,  2.43700340e-01,\n",
       "        -4.74591330e-02, -1.52507514e-01, -1.56531617e-01, -7.70053864e-02,\n",
       "         3.23678821e-01,  5.87066114e-02,  1.75740749e-01, -2.52255887e-01,\n",
       "         1.31559834e-01,  1.45151258e-01, -3.38768065e-01,  4.70615774e-01,\n",
       "        -8.79674107e-02, -8.02056938e-02, -2.82313973e-01, -7.97221884e-02,\n",
       "         4.08975899e-01, -4.22805458e-01,  1.46422356e-01,  2.24162132e-01,\n",
       "         4.04123008e-01,  4.81076054e-02, -1.53551161e-01,  1.81611449e-01,\n",
       "        -3.91834825e-02, -1.93264887e-01,  2.89571285e-02,  4.76494282e-02,\n",
       "         1.49231758e-02,  2.03564048e-01, -2.72623658e-01, -3.46618086e-01,\n",
       "        -3.46739084e-01, -1.32596642e-01,  1.48504212e-01, -3.99900138e-01,\n",
       "         1.17956661e-01, -6.18848726e-02, -3.45708728e-01,  2.75584906e-01,\n",
       "        -1.62469894e-01, -1.11133471e-01,  1.18935004e-01,  1.11912012e-01,\n",
       "        -4.31993484e-01, -2.15894490e-01,  4.65492234e-02,  2.87502562e-03,\n",
       "        -1.10232450e-01, -1.60419822e-01,  5.33675849e-02, -5.57937801e-01,\n",
       "         2.27863386e-01,  2.29525611e-01, -1.82325542e-01,  1.78409666e-01,\n",
       "         7.68676549e-02, -2.31605813e-01,  3.92185990e-03, -1.02563679e-01,\n",
       "        -1.32953495e-01,  7.10101575e-02, -4.60225902e-02, -4.66021448e-01,\n",
       "         1.40842602e-01,  8.71931762e-02, -9.16909426e-02,  4.54757035e-01,\n",
       "        -3.14764798e-01,  4.11070526e-01,  8.34114924e-02,  5.65460250e-02,\n",
       "        -1.90896556e-01, -3.61523241e-01,  1.53028831e-01, -5.80728948e-01,\n",
       "        -4.14395481e-01,  1.40489042e-01, -2.15945482e-01,  2.53703110e-02,\n",
       "        -3.70940045e-02, -8.07945281e-02,  2.43332181e-02,  1.95657790e-01,\n",
       "         3.91453877e-02,  1.45828679e-01,  1.52235776e-01,  2.65965968e-01,\n",
       "         8.78299307e-03, -3.35776716e-01, -6.32462725e-02, -1.25887707e-01,\n",
       "         1.95193857e-01, -1.99491486e-01, -3.08177527e-02,  5.71009591e-02,\n",
       "        -2.63492167e-01, -3.69253963e-01,  4.35189903e-01, -3.08350384e-01,\n",
       "        -9.93868932e-02,  8.41958895e-02,  9.33711082e-02, -2.21191794e-01,\n",
       "         2.59506881e-01, -2.70524651e-01, -3.05956099e-02,  2.76049018e-01,\n",
       "        -1.92995399e-01, -1.13301538e-01,  2.10063413e-01,  1.73417509e-01,\n",
       "        -1.72359385e-02,  5.68221100e-02,  4.75977771e-02,  7.23413751e-02,\n",
       "         4.28611666e-01, -3.37923467e-02,  1.39286682e-01,  7.28887767e-02,\n",
       "        -1.25078708e-01, -1.21722162e-01,  2.12954789e-01,  2.47170568e-01,\n",
       "        -1.19792737e-01,  1.78902522e-01,  1.35096520e-01, -3.56406242e-01,\n",
       "         2.49430060e-01,  3.01446676e-01, -2.03494266e-01, -3.65906775e-01,\n",
       "         6.46808863e-01,  3.97900343e-01, -3.10267329e-01, -3.12562466e-01,\n",
       "         5.89193031e-02, -2.51687437e-01, -8.61794204e-02,  4.12899889e-02,\n",
       "         8.34480673e-02, -7.30630606e-02,  5.80847621e-01, -1.38518093e-02,\n",
       "         1.65398717e-01,  2.93454438e-01, -3.39541346e-01, -1.09286703e-01,\n",
       "         5.61669236e-03,  2.52421200e-01, -4.51759622e-02,  1.88701972e-01,\n",
       "        -2.52959907e-01,  4.95094478e-01, -2.28485346e-01, -2.07995057e-01,\n",
       "        -2.67861108e-03, -3.05062942e-02,  1.00461267e-01,  9.21657085e-02,\n",
       "        -6.34903982e-02,  1.51218876e-01,  4.14110184e-01,  4.87201154e-01,\n",
       "         2.38008976e-01,  1.95629105e-01,  1.24234714e-01,  2.12540315e-03,\n",
       "         8.09295774e-01,  4.45679009e-01,  1.95373163e-01,  4.60265458e-01,\n",
       "        -6.83439225e-02, -1.30839616e-01,  7.25210384e-02,  3.17525387e-01,\n",
       "         2.51223892e-01,  2.29695439e-01, -1.28027529e-01,  4.83585358e-01,\n",
       "         1.31456316e-01, -4.18515503e-02,  7.95353204e-02, -3.90046537e-01,\n",
       "        -3.98930609e-02,  1.74901187e-01,  1.84470162e-01, -1.98777199e-01,\n",
       "        -2.54465163e-01, -1.37386620e-02, -5.51119745e-02, -9.56955552e-03,\n",
       "        -2.37896964e-01, -9.86204818e-02, -2.92335898e-01,  4.20442969e-01,\n",
       "         1.41207650e-01, -1.27070591e-01, -2.07114786e-01,  2.56158300e-02,\n",
       "        -5.54367229e-02, -2.44817398e-02, -3.83389056e-01, -1.15967572e-01,\n",
       "        -1.36471450e-01, -6.73626542e-01, -6.97306544e-02,  1.32246450e-01,\n",
       "        -1.80933535e-01, -3.69618356e-01, -6.26487061e-02, -9.01722088e-02,\n",
       "         6.51987940e-02, -2.65649259e-01,  3.26075852e-02, -2.91857958e-01,\n",
       "         3.78874868e-01,  2.79272854e-01,  2.87051797e-01,  1.48551464e-01,\n",
       "         1.52791142e-01, -2.09955648e-01, -1.87998712e-02,  1.22605205e-01,\n",
       "         1.41415402e-01,  9.90783125e-02, -3.33386585e-02,  2.74256885e-01,\n",
       "        -3.19320738e-01, -4.08433899e-02,  2.70931814e-02,  5.49965441e-01,\n",
       "        -7.72988856e-01,  1.98939025e-01, -1.19555913e-01, -7.90752918e-02,\n",
       "         1.31273016e-01,  7.36306161e-02, -2.85377920e-01,  3.24106693e-01,\n",
       "        -3.84590030e-01, -2.85868019e-01, -1.44402860e-02,  1.60764307e-01,\n",
       "        -1.96643993e-01, -2.22821981e-02,  1.12799488e-01,  4.07928169e-01,\n",
       "        -8.29117820e-02, -1.61297604e-01,  1.64029300e-02,  2.31751770e-01,\n",
       "         7.89752752e-02,  1.90674327e-02, -7.20960423e-02, -2.89646059e-01,\n",
       "        -2.34760493e-01,  5.07469615e-03, -1.72230244e-01, -7.27295876e-04,\n",
       "         2.49115467e-01,  1.06521644e-01,  2.53504273e-02, -5.93503602e-02,\n",
       "         3.37525427e-01, -1.02008283e-01,  6.38322830e-02, -1.15744904e-01,\n",
       "        -1.32094055e-01, -3.98623526e-01, -7.73665234e-02, -5.69402389e-02,\n",
       "         4.08030823e-02, -2.26465076e-01,  4.53211702e-02, -7.36198947e-02,\n",
       "        -1.58149488e-02, -2.52808005e-01,  2.02506229e-01,  1.72768496e-02],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_corpora_vecs = [corpora_vector(x, model_embedding, my_tokenizer) for x in my_corpora]\n",
    "my_corpora_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.0234375 , 0.02734375, 0.01953125],\n",
       "       [0.0234375 , 0.        , 0.0234375 , 0.02083333],\n",
       "       [0.02734375, 0.0234375 , 0.        , 0.02213542],\n",
       "       [0.01953125, 0.02083333, 0.02213542, 0.        ]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import scipy as sp \n",
    "def diff_matrix_plot(corpora_vecs):\n",
    "    L = []\n",
    "    for p in corpora_vecs:\n",
    "        l = []\n",
    "        for q in corpora_vecs:\n",
    "            l.append(sp.stats.ks_2samp(p,q)[0])\n",
    "        L.append(l)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(np.array(L),columns = range(1,5), index = range(1,5))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()\n",
    "    return np.array(L)\n",
    "diff_matrix_plot(my_corpora_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's compare it to the KS divergence result based on word frequency in HW4 below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two heatmaps, we can see some difference between the results of Bret embedding based KL distance analysis and that of word frequency based KL distance analysis. Corpora 4 has a longer distance to Corpora 3 compared with to corpora 1 and 2 in Bret'S analysis, but becomes closest to Corpora 3 in the word frequency analysis. This tiny difference indicates that external word embeddings might give us more information than what we can gain from the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation using BERT\n",
    "\n",
    "The last method which we will explore is text generation. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
    "\n",
    "These models can be quite impressive, even uncanny in how human like they sound. Check out this [cool website](https://transformer.huggingface.co), which allows you to write with a transformer. The website is built by the folks who wrote the package we are using. The code underneath the website can be found in their examples: [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py).\n",
    "\n",
    "We will be using the built in generate function, but the example file has more detailed code which allows you to set the seed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9e9746c37d4efaab41559046aae232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing that we like to do more than analyse data all day long and read an article. We've just got to wait and see. Some people are going to look at a bit of that and say I've got to buy another one and sell it\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Nothing that we like to do more than analyse data all day long and\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. A little creepy, and as we can see, far from perfect: GPT doesn't alwats work out flawlessly, but it sometimes can, and we will try and see if fine-tuning helps. We are going to tune the model on a complete dataset of Trump tweets, as they have a set of distinctive, highly identifiable qualities.\n",
    "\n",
    "### Creating a domain-specific language model\n",
    "\n",
    "One of the most exciting things about BERT and GPT is being able to retune them the way we want to. We will be training models to perform two tasks - one is to create a BERT with an \"accent\", by traning a model with english news data from the UK, from the US, and from India. We will also train a language generation model with a bunch of Trump tweets. \n",
    "\n",
    "We can train models specifically over a certain domain to make its language generation similar to that domain. \n",
    "[run_language modelling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), followed by [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py). I've downloaded these files and added them to this directory so we can run them through the notebook. You are encouraged to look at these files to get a rough idea of what is going on.\n",
    "\n",
    "### Loading Data \n",
    "\n",
    "We want to now get our Trump tweets and our English news datasets ready. The data the scripts expect is just a text file with relevant data. We load the Trump tweets and then write them to disk as train and test files with only data. I leave the original dataframes in case you would like to use it for your own purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(\"../data/trump_tweets\"):\n",
    "    dfs.append(pd.read_json(\"../data/trump_tweets/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>550441250965708800</td>\n",
       "      <td>\"@ronmeier123: @Macys Your APPAREL is UNPARALL...</td>\n",
       "      <td>2014-12-31 23:59:55+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>550441111513493504</td>\n",
       "      <td>\"@gillule4: @realDonaldTrump incredible experi...</td>\n",
       "      <td>2014-12-31 23:59:22+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>550440752254562304</td>\n",
       "      <td>\"@JobSnarechs: Negotiation tip #1: The worst t...</td>\n",
       "      <td>2014-12-31 23:57:56+00:00</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>550440620792492032</td>\n",
       "      <td>\"@joelmch2os: @realDonaldTrump announce your p...</td>\n",
       "      <td>2014-12-31 23:57:25+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>550440523094577152</td>\n",
       "      <td>\"@djspookyshadow: Feeling a deep gratitude for...</td>\n",
       "      <td>2014-12-31 23:57:02+00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                source              id_str  \\\n",
       "0  Twitter for Android  550441250965708800   \n",
       "1  Twitter for Android  550441111513493504   \n",
       "2  Twitter for Android  550440752254562304   \n",
       "3  Twitter for Android  550440620792492032   \n",
       "4  Twitter for Android  550440523094577152   \n",
       "\n",
       "                                                text  \\\n",
       "0  \"@ronmeier123: @Macys Your APPAREL is UNPARALL...   \n",
       "1  \"@gillule4: @realDonaldTrump incredible experi...   \n",
       "2  \"@JobSnarechs: Negotiation tip #1: The worst t...   \n",
       "3  \"@joelmch2os: @realDonaldTrump announce your p...   \n",
       "4  \"@djspookyshadow: Feeling a deep gratitude for...   \n",
       "\n",
       "                 created_at  retweet_count  in_reply_to_user_id_str  \\\n",
       "0 2014-12-31 23:59:55+00:00              8                      NaN   \n",
       "1 2014-12-31 23:59:22+00:00              5                      NaN   \n",
       "2 2014-12-31 23:57:56+00:00             33                      NaN   \n",
       "3 2014-12-31 23:57:25+00:00              8                      NaN   \n",
       "4 2014-12-31 23:57:02+00:00              9                      NaN   \n",
       "\n",
       "   favorite_count  is_retweet  \n",
       "0              21       False  \n",
       "1              18       False  \n",
       "2              44       False  \n",
       "3              26       False  \n",
       "4              31       False  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text = train_test_split(df['text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67            \"@fackinpeter: @realDonaldTrump Trump 2016\"\n",
       "1710    Heading to the Great State of Wisconsin to tal...\n",
       "2783    \"@AceWill Just bought @realDonaldTrump's: Thin...\n",
       "3276    \"@DustinDeMoss: @realDonaldTrump I disagree wi...\n",
       "633     Virtually no-one has spent more money in helpi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.to_frame().to_csv(r'train_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text.to_frame().to_csv(r'test_text_trump', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now used the Google Colab GPUs to train the Trump tweet models. We'll be doing the same for our blog posts too.\n",
    "\n",
    "### GloWBe dataset\n",
    "\n",
    "We'll now load up the GloWbe (Corpus of Global Web-Based English) dataset which have different texts from different countries. We'll try and draw out texts from only the US, UK and India. We'll then save these to disk. Note that this is a Davies Corpora dataset: the full download can be done with the Dropbox link I sent in an announcement a few weeks ago. The whole download is about 3.5 GB but we only need two files, which are anout 250 MB each. The other files might be useful for your research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucem_illud_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"/Users/bhargavvader/Downloads/Academics_tech/corpora/GloWbE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the exact name of the files\n",
    "us = \"/text_us_blog_jfy.zip\"\n",
    "gb = \"/text_gb_blog_akq.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_us_blog_jfy.zip\n"
     ]
    }
   ],
   "source": [
    "us_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"us_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_gb_blog_akq.zip\n"
     ]
    }
   ],
   "source": [
    "gb_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"gb_blog\", num_files=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dictionary with document ids mapping to text. Since we don't need any information but the text, we can just save these to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< p > Many workers within the UK are required to wear an uniform for work but very few are aware they could really claim back some money from the tax man to help with the price of washing or repairing the uniform HMRC will actually pay money back once again to those individuals who are eligible even dating back four years worth of washing < p > In order to find out about claiming a tax rebate on uniform it is worth having a look online to find out whether or not you will be eligible The conditions are fairly straightforward you just have to wear a recognisable work uniform which might contain a T shirt which displays a logo design or possibly some specialist protective clothing The type of the occupation is unrelated anyone from nurses to cops to electricians are able to claim As long as the uniform is worn at work is washed on your own and you are an UK tax payer then your chances are you will be eligible to claim are eligible how do you actually go about claiming your tax refund for washing uniform You can access the HMRC web site directly and complete the important forms to help you to claim your hard earned money back Or you can use an agent that is registered with the HMRC who are able to deal with the whole tax rebate on your behalf This may save you a great deal of time and effort and they may also have the ability to advise you on any other areas where you could claim a rebate for instance if you are required to supply your own tools within your job < p > The amount of money you can claim as a tax rebate on uniform will depend on your profession and how long you have already been wearing a work uniform for You might be in a position to claim a rebate for the past four years which could add up to a fine sum of money < p > In those times of economic decline most people are seeking to save lots of washing uniform can be an effective way to start saving a couple of pounds every month which over the course of a'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(list(us_texts.values())[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_texts(texts, file_name):\n",
    "    text = []\n",
    "    for doc in list(texts.values()):\n",
    "        text.append(' '.join(doc).replace(\"< h >\", \"\").replace(\"< p >\", \"\"))\n",
    "    train_text, test_text = train_test_split(text, test_size=0.2)\n",
    "    with open(file_name + \"_train\", 'w') as f:\n",
    "        for item in train_text:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(file_name + \"_test\", 'w') as f:\n",
    "        for item in test_text:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_texts(us_texts, \"us_blog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_texts(gb_texts, \"gb_blog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the training and testing files for both US and GB blogs in English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WARNING - SHIFT TO GOOGLE COLAB\n",
    "\n",
    "The [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) walks you through the process of fine-tuning models, as we did before for the classification task. Move now to the colab file to fine tune your models. Once you downloaded all the models and their information, place those files in the directory of the HW to use them as demonstrated below. \n",
    "\n",
    "\n",
    "\n",
    "### Running Scripts\n",
    "\n",
    "We use the scripts to do language modelling and text generation. The following cells run the code as if you would have run it in a terminal. I trained all of these models using the Googlr Colab file, and then saved the models to disk.\n",
    "\n",
    "#### Trump GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run_language_modelling.py --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=train_text_trump --do_eval --eval_data_file=test_text_trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run_language_modeling.py --output_dir=output_roberta_UK --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and using models\n",
    "\n",
    "Let us now load the four models we have and see how we can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now - let us see what our Trump Tweet Bot looks like!\n",
    "You can generate text via command line using the command below. You can also load a model once it is saved - I trained my model using Google Colab, downloaded the model, and am loading it again via the command below. Note that you have to download all the files in your folder of the fine-tuned model to use the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run_generation.py --model_type=gpt2 --model_name_or_path=output_trump_gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_trump = AutoTokenizer.from_pretrained(\"output_trump_gpt\")\n",
    "model_trump = AutoModelWithLMHead.from_pretrained(\"output_trump_gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a disaster for the United States. He is a total loser. He is a total loser!\"\n",
      "\"\"\"@jeff_mcclaren: @realDonaldTrump @realDonaldTrump @foxandfriends @megynkelly @\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - our Trump bot is nasty, so we know our model trained well. What happens if we try the same sentence for our non-fine tuned model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama is going to be a very good president,\" said Sen. John McCain (R-Ariz.). \"He's going to be a very good president. He's going to be a very good president. He's going to be a very\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Obama is going to\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite the contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 3*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading new corpora from IG webpage\n",
    "toxic_df = my_df.loc[my_df['toxic']==1.0]\n",
    "normal_df = my_df.loc[my_df['toxic']!=1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6          cock  suck before you piss around on my work\n",
       "12    hey  what is it talk what is it  an exclusive ...\n",
       "16    bye  don t look  come or think of comming back...\n",
       "42    you are  gay  or antisemmitian archangel white...\n",
       "43             fuck your filthy mother in the ass  dry \n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments = toxic_df['comment_text']\n",
    "toxic_comments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explanation why the edits made under my userna...\n",
       "1    d aww  he matches this background colour i m s...\n",
       "2    hey man  i m really not trying to edit war  it...\n",
       "3      more i can t make any real suggestions on im...\n",
       "4    you  sir  are my hero  any chance you remember...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_comments = normal_df['comment_text']\n",
    "normal_comments[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "toxic_train, toxic_test = train_test_split(toxic_comments, test_size=0.1)\n",
    "normal_train, normal_test = train_test_split(normal_comments, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_train.to_frame().to_csv(r'toxic_train', header=None, index=None, sep=' ', mode='a')\n",
    "toxic_test.to_frame().to_csv(r'toxic_test', header=None, index=None, sep=' ', mode='a')\n",
    "normal_train.to_frame().to_csv(r'normal_train', header=None, index=None, sep=' ', mode='a')\n",
    "normal_test.to_frame().to_csv(r'normal_test', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_post = \"I do not like it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I don\\'t like it, we should work harder and look at it, in theory.\\n\\n\"People have been asking me if this is a good thing, for good reasons, but I will be there to talk to them. The real question'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see the result of a untrained model\n",
    "untrained_tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "untrained_model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n",
    "\n",
    "my_input = untrained_tokenizer_gpt.encode(test_post, return_tensors=\"pt\")\n",
    "generated = untrained_model_gpt.generate(my_input, max_length=50)\n",
    "\n",
    "untrained_tokenizer_gpt.decode(generated.tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a little bit like a reply from online social media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on google colab\n",
    "!python /content/run_language_modelling.py --output_dir=output_gpt_toxic --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/toxic_train --do_eval --eval_data_file=/content/toxic_test --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_tokenizer = AutoTokenizer.from_pretrained(\"toxic_model\")\n",
    "toxic_model = AutoModelWithLMHead.from_pretrained(\"toxic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't like it.\\n\\nYou have always had the worst feelings about me personally right?\\n\\nDid you even get to the point of getting upset with me before me getting angry with you?\\n\\nIt's not an accident you made\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_input = toxic_tokenizer.encode(test_post, return_tensors=\"pt\")\n",
    "generated = toxic_model.generate(my_input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "toxic_tokenizer.decode(generated.tolist()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the toxic dataset trained set makes our generated text much more aggresive, so maybe some online comment attackers would use such techniques to produce large sets of bullying messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check out our UK and GB embeddings - how do you think the two models will differ? Maybe in the way different words relate to each other in the same sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_us_model_embedding = RobertaModel.from_pretrained('roberta_us')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('roberta_us')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualise how words in a sentence or different or similar to each other. We will try to construct sentences where words might mean different things in different countries - in the US, people might eat chips with salsa, but in the UK, chips are what Americans call french fries, and might eat it fried fish instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Do you have your chips with fish or with salsa?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"He went out in just his undershirt and pants.\" #pants are underwear in Britain; maybe closer to an undershirt\n",
    "text2 = \"His braces completed the outfit.\" #braces are suspenders (in Britain); maybe closer to an outfit\n",
    "text3 = \"Does your pencil have a rubber on it?\" #rubber is an eraser in Britain); maybe closer to a pencil\n",
    "text4 = \"Was the bog closer to the forest or the house?\" #bog is a toilen in Britain); maybe closer to a house\n",
    "text5 = \"Are you taking the trolley or the train to the grocery market\" #trolley is a food carriage; possibly closer to a market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_diffs(text, model, tokenizer):\n",
    "    word_vecs = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
    "    L = []\n",
    "    for p in word_vecs:\n",
    "        l = []\n",
    "        for q in word_vecs:\n",
    "            l.append(1 - cosine(p, q))\n",
    "        L.append(l)\n",
    "    M = np.array(L)\n",
    "    fig = plt.figure()\n",
    "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
    "    ax = sns.heatmap(div)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_us_model_embedding, roberta_us_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_gb_model_embedding = RobertaModel.from_pretrained('roberta_gb')\n",
    "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('roberta_gb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualise_diffs(text, roberta_gb_model_embedding, roberta_gb_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see regarding the relations with chips and sala/fish? What about the other sentences? How about comparing sentence embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 4*</span>\n",
    "\n",
    "<span style=\"color:red\">Construct cells immediately below this that tune BERT to at least two different textual samples. These could be from different corpora, distinct time periods, separate authors, alternative publishing outlets, etc. Then compare the meaning of words, phrases and sentences to each other across the separate models. What do they reveal about the social worlds inscribed by the distinctive samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 2 trained models\n",
    "toxic_model = RobertaModel.from_pretrained('toxic_model')\n",
    "toxic_tokenizer = RobertaTokenizer.from_pretrained('toxic_model')\n",
    "normal_model = RobertaModel.from_pretrained('normal_model')\n",
    "normal_tokenizer = RobertaTokenizer.from_pretrained('normal_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' congratulations from me as well  use the tools well  talk  '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_comment = my_df['comment_text'][5]\n",
    "one_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10      fair use rationale for image wonju jpg  than...\n",
       "11    bbq be a man and lets discuss it maybe over th...\n",
       "12    hey  what is it talk what is it  an exclusive ...\n",
       "13    before you start throwing accusations and warn...\n",
       "14    oh  and the girl above started her arguments w...\n",
       "15     juelz santanas age  in juelz santana was year...\n",
       "16    bye  don t look  come or think of comming back...\n",
       "17     redirect talk voydan pop georgiev  chernodrinski\n",
       "18    the mitsurugi point made no sense why not argu...\n",
       "19    don t mean to bother you i see that you re wri...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_comments = my_df['comment_text'][10:20]\n",
    "multiple_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2821\n",
      "54\n",
      "301\n",
      "815\n",
      "219\n",
      "546\n",
      "55\n",
      "48\n",
      "116\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "for i in multiple_comments:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_compare(model1, model2, text, tokenizer):\n",
    "    word_vecs1 = []\n",
    "    word_vecs2 = []\n",
    "    cos_scores = []\n",
    "    for i in range(0, len(text.split())):\n",
    "        vec1 = word_vector(text, i, model1, tokenizer)\n",
    "        vec2 = word_vector(text, i, model2, tokenizer)\n",
    "        word_vecs1.append(vec1)\n",
    "        word_vecs2.append(vec2)\n",
    "        cos_scores.append(cosine(vec1,vec2))\n",
    "    plt.bar(text.split(), cos_scores)\n",
    "    plt.ylabel('cosine-distance')\n",
    "    plt.title('word comparison')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_compare(model1, model2, sents, tokenizer):\n",
    "    cos_scores = []\n",
    "    for sent in sents:\n",
    "        try:\n",
    "            vec1 = sentence_vector(sent, model1, tokenizer)\n",
    "            vec2 = sentence_vector(sent, model2, tokenizer)\n",
    "            cos_scores.append(cosine(vec1,vec2))\n",
    "        except:\n",
    "            continue\n",
    "    print(cos_scores)\n",
    "    plt.bar(range(len(cos_scores)), cos_scores)\n",
    "    plt.ylabel('cosine-distance')\n",
    "    plt.title('sentence comparison')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_compare(toxic_model, normal_model, one_comment, normal_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9885999923571944, 1.0255017466843128, 1.0222662538290024, 1.0035030141007155, 1.0201071985065937, 1.0058390451595187, 1.0014198401477188, 1.0224544238299131, 1.0157119277864695]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_compare(toxic_model, normal_model, multiple_comments, normal_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two plots, we can see that both the word-embeddings and sentence-embeddings trained from the toxic and normal corpora are quite different. Even though the cosine distance obtained by the same corpus is still very large, it shows that in normal and toxic comments, the same words can have very large differences. Therefore, if a single word is selected as the feature that distinguishes the two type of comments, I think it is necessary to select words that have similar meanings in two environments or only appear in one of them. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
